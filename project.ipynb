{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import skimage.io\n",
    "from scipy.ndimage.filters import convolve\n",
    "\n",
    "#note: this requires the starter code for the assignments!\n",
    "from common.plotting import plot_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780 (CNMeM is enabled)\n",
      "/home/i265983/Dokumenty/nn_assignments/libs/Theano/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor.signal.downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "def svgdotprint(g):\n",
    "    return SVG(theano.printing.pydotprint(g, return_image=True, format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "CIFAR10.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}))\n",
    "\n",
    "cifar10_train = CIFAR10((\"train\",), subset=slice(None,40000))\n",
    "#this stream will shuffle the CIFAR10 set and return us batches of 100 examples\n",
    "cifar10_train_stream = DataStream.default_stream(\n",
    "    cifar10_train,\n",
    "    iteration_scheme=ShuffledScheme(cifar10_train.num_examples, 25))\n",
    "                                               \n",
    "cifar10_validation = CIFAR10((\"train\",), subset=slice(40000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "cifar10_validation_stream = DataStream.default_stream(\n",
    "    cifar10_validation, iteration_scheme=SequentialScheme(cifar10_validation.num_examples, 100))\n",
    "cifar10_test = CIFAR10((\"test\",))\n",
    "cifar10_test_stream = DataStream.default_stream(\n",
    "    cifar10_test, iteration_scheme=SequentialScheme(cifar10_test.num_examples, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (25, 3, 32, 32) containing float32\n",
      " - an array of size (25, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (cifar10_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar10_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar10_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    n = 1\n",
    "    def __init__(self, rng=None):\n",
    "        if rng == None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "        self._parameters = []\n",
    "        self.n = Layer.n\n",
    "        Layer.n += 1\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return self._parameters\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        return X\n",
    "    \n",
    "    def name(self):\n",
    "        return \"Base layer\"\n",
    "\n",
    "\n",
    "class Conv(Layer):\n",
    "    def __init__(self, num_in, num_out, filter_size=5, weight_init=None, bias_init=None, **kwargs):\n",
    "        super(Conv, self).__init__(**kwargs)\n",
    "        if weight_init == None:\n",
    "            weight_init = IsotropicGaussian(0.05)\n",
    "        if bias_init == None:\n",
    "            bias_init = Constant(0.0)\n",
    "        \n",
    "        self.weight_init = weight_init\n",
    "        self.bias_init = bias_init\n",
    "        self.num_in = num_in\n",
    "        self.num_out = num_out\n",
    "        self.filter_size = filter_size\n",
    "            \n",
    "        self.W = theano.shared(np.zeros((self.num_out, self.num_in, self.filter_size, self.filter_size), dtype='float32'),\n",
    "                               name='CW'+str(self.n))\n",
    "        self.W.tag.initializer = self.weight_init\n",
    "        self.B = theano.shared(np.zeros((self.num_out,), dtype='float32'),\n",
    "                               name='CB'+str(self.n))\n",
    "        self.B.tag.initializer = self.bias_init\n",
    "        self._parameters = [self.W, self.B]\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        return theano.tensor.nnet.conv2d(X, self.W, border_mode='full') + self.B.dimshuffle('x',0,'x','x')\n",
    "    \n",
    "    def name(self):\n",
    "        return \"Convolution layer (%d, %d, %d)\" % (self.num_in, self.num_out, self.filter_size)\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReLU, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        return theano.tensor.maximum(0.0, X)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"ReLU layer\"\n",
    "    \n",
    "class MaxPool(Layer):\n",
    "    def __init__(self, filter_size=2, **kwargs):\n",
    "        super(MaxPool, self).__init__(**kwargs)\n",
    "        self.filter_size = filter_size\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        return theano.tensor.signal.downsample.max_pool_2d(X, (self.filter_size,self.filter_size), ignore_border=True)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"Max pooling layer (%d x %d)\" % (self.filter_size, self.filter_size)\n",
    "    \n",
    "\n",
    "class Flatten(Layer):\n",
    "    def __init__(self, dim, **kwargs):\n",
    "        super(Flatten, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        return X.flatten(self.dim)\n",
    "\n",
    "    def name(self):\n",
    "        return \"Flatten layer (%d)\" % (self.dim)\n",
    "    \n",
    "class Affine(Layer):\n",
    "    def __init__(self, num_in, num_out, weight_init=None, bias_init=None, **kwargs):\n",
    "        super(Affine, self).__init__(**kwargs)\n",
    "        if weight_init == None:\n",
    "            weight_init = IsotropicGaussian(std=2./np.sqrt(num_in))\n",
    "        if bias_init == None:\n",
    "            bias_init = Constant(0.0)\n",
    "        \n",
    "        self.weight_init = weight_init\n",
    "        self.bias_init = bias_init\n",
    "        self.num_in = num_in\n",
    "        self.num_out = num_out\n",
    "            \n",
    "        self.W = theano.shared(np.zeros((self.num_in, self.num_out), dtype='float32'),\n",
    "                               name='AW'+str(self.n))\n",
    "        self.W.tag.initializer = self.weight_init\n",
    "        self.B = theano.shared(np.zeros((self.num_out,), dtype='float32'),\n",
    "                               name='AB'+str(self.n))\n",
    "        self.B.tag.initializer = self.bias_init\n",
    "        self._parameters = [self.W, self.B]\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        return theano.tensor.dot(X, self.W) + self.B.dimshuffle('x',0)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"Affine layer (%d, %d)\" % (self.num_in, self.num_out)\n",
    "\n",
    "    \n",
    "class SoftMax(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftMax, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        return theano.tensor.nnet.softmax(X)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"Softmax layer\"\n",
    "\n",
    "    \n",
    "class Predict(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Predict, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        return theano.tensor.argmax(X, axis=1)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"Prediction layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None):\n",
    "        if layers == None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "        self.log_probs = None\n",
    "        self.predictions = None\n",
    "        self.error_rate = None\n",
    "        self.nll = None\n",
    "        self.weight_decay = None\n",
    "        self.cost = None\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    def initialize(self, X, wdec_const):\n",
    "        theano.config.compute_test_value = 'off'\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            X = layer.fprop(X)\n",
    "        \n",
    "        self.log_probs = theano.tensor.nnet.softmax(X)\n",
    "        self.predictions = theano.tensor.argmax(self.log_probs, axis=1)\n",
    "        self.error_rate = theano.tensor.neq(self.predictions,Y.ravel()).mean()\n",
    "        self.nll = - theano.tensor.log(self.log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "        self.weight_decay = 0.0\n",
    "        for p in self.parameters:\n",
    "            if p.name[1]=='W':\n",
    "                self.weight_decay = self.weight_decay + wdec_const * (p**2).sum()\n",
    "        self.cost = self.nll + self.weight_decay\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = theano.tensor.tensor4('X')\n",
    "Y = theano.tensor.matrix('Y', dtype='uint8')\n",
    "X_test_value, Y_test_value = next(cifar10_train_stream.get_epoch_iterator())\n",
    "X.tag.test_value = X_test_value[:3]\n",
    "Y.tag.test_value = Y_test_value[:3]\n",
    "\n",
    "net = FeedForwardNet(layers=[\n",
    "        Conv(3, 40, filter_size=5),   ReLU(), MaxPool(2),\n",
    "        Conv(40, 100, filter_size=5), ReLU(), MaxPool(2),\n",
    "        Flatten(2),\n",
    "        Affine(100 * 11 * 11, 800),\n",
    "        ReLU(),\n",
    "        Affine(800, 10)\n",
    "    ])\n",
    "\n",
    "net.initialize(X, wdec_const=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(V_CW1, Elemwise{sub,no_inplace}.0),\n",
       " (CW1, Elemwise{add,no_inplace}.0),\n",
       " (V_CB1, Elemwise{sub,no_inplace}.0),\n",
       " (CB1, Elemwise{add,no_inplace}.0),\n",
       " (V_CW4, Elemwise{sub,no_inplace}.0),\n",
       " (CW4, Elemwise{add,no_inplace}.0),\n",
       " (V_CB4, Elemwise{sub,no_inplace}.0),\n",
       " (CB4, Elemwise{add,no_inplace}.0),\n",
       " (V_AW8, Elemwise{sub,no_inplace}.0),\n",
       " (AW8, Elemwise{add,no_inplace}.0),\n",
       " (V_AB8, Elemwise{sub,no_inplace}.0),\n",
       " (AB8, Elemwise{add,no_inplace}.0),\n",
       " (V_AW10, Elemwise{sub,no_inplace}.0),\n",
       " (AW10, Elemwise{add,no_inplace}.0),\n",
       " (V_AB10, Elemwise{sub,no_inplace}.0),\n",
       " (AB10, Elemwise{add,no_inplace}.0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The updates will update our shared values\n",
    "updates = []\n",
    "\n",
    "lrate = theano.tensor.scalar('lrate',dtype='float32')\n",
    "momentum = theano.tensor.scalar('momentum',dtype='float32')\n",
    "\n",
    "# Theano will compute the gradients for us\n",
    "gradients = theano.grad(net.cost, net.parameters)\n",
    "\n",
    "#initialize storage for momentum\n",
    "velocities = [theano.shared(np.zeros_like(p.get_value()), name='V_%s' %(p.name, )) for p in net.parameters]\n",
    "\n",
    "for p,g,v in zip(net.parameters, gradients, velocities):\n",
    "    v_new = momentum * v - lrate * g\n",
    "    p_new = p + v_new\n",
    "    updates += [(v,v_new), (p, p_new)]\n",
    "\n",
    "updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compile theano functions\n",
    "\n",
    "#each call to train step will make one SGD step\n",
    "train_step = theano.function([X,Y,lrate,momentum],[net.cost, net.error_rate, net.nll, net.weight_decay],updates=updates)\n",
    "#each call to predict will return predictions on a batch of data\n",
    "predict = theano.function([X], net.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(stream):\n",
    "    errs = 0.0\n",
    "    num_samples = 0.0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        errs += (predict(X)!=Y.ravel()).sum()\n",
    "        num_samples += Y.shape[0]\n",
    "    return errs/num_samples\n",
    "\n",
    "\n",
    "#utilities to save values of parameters and to load them\n",
    "\n",
    "def init_parameters():\n",
    "    rng = np.random.RandomState(1234)\n",
    "    for p in net.parameters:\n",
    "        p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "\n",
    "def snapshot_parameters():\n",
    "    return [p.get_value(borrow=False) for p in net.parameters]\n",
    "\n",
    "def load_parameters(snapshot):\n",
    "    for p, s in zip(net.parameters, snapshot):\n",
    "        p.set_value(s, borrow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 16.248087, batch nll 1.604274, batch error rate 56.000000%\n",
      "At minibatch 200, batch loss 14.200373, batch nll 2.183568, batch error rate 80.000000%\n",
      "At minibatch 300, batch loss 11.726818, batch nll 1.858489, batch error rate 64.000000%\n",
      "At minibatch 400, batch loss 9.480042, batch nll 1.370698, batch error rate 40.000000%\n",
      "At minibatch 500, batch loss 8.211007, batch nll 1.531607, batch error rate 60.000000%\n",
      "At minibatch 600, batch loss 7.165692, batch nll 1.657004, batch error rate 60.000000%\n",
      "At minibatch 700, batch loss 5.899021, batch nll 1.343882, batch error rate 48.000000%\n",
      "At minibatch 800, batch loss 5.757194, batch nll 1.982134, batch error rate 72.000000%\n",
      "At minibatch 900, batch loss 4.428487, batch nll 1.290509, batch error rate 60.000000%\n",
      "At minibatch 1000, batch loss 3.984294, batch nll 1.361165, batch error rate 48.000000%\n",
      "At minibatch 1100, batch loss 3.655833, batch nll 1.457846, batch error rate 48.000000%\n",
      "At minibatch 1200, batch loss 3.187401, batch nll 1.332413, batch error rate 48.000000%\n",
      "At minibatch 1300, batch loss 2.780398, batch nll 1.206471, batch error rate 32.000000%\n",
      "At minibatch 1400, batch loss 2.822337, batch nll 1.474484, batch error rate 52.000000%\n",
      "At minibatch 1500, batch loss 2.374660, batch nll 1.215027, batch error rate 52.000000%\n",
      "At minibatch 1600, batch loss 2.178406, batch nll 1.166457, batch error rate 40.000000%\n",
      "After epoch 1: valid_err_rate: 41.760000% currently going to do 3 epochs\n",
      "After epoch 1: averaged train_err_rate: 51.155000% averaged train nll: 1.422836 averaged train loss: 6.928469\n",
      "At minibatch 1700, batch loss 1.921583, batch nll 1.025910, batch error rate 36.000000%\n",
      "At minibatch 1800, batch loss 1.834064, batch nll 1.039361, batch error rate 36.000000%\n",
      "At minibatch 1900, batch loss 1.911738, batch nll 1.191484, batch error rate 48.000000%\n",
      "At minibatch 2000, batch loss 2.284053, batch nll 1.630805, batch error rate 56.000000%\n",
      "At minibatch 2100, batch loss 1.927067, batch nll 1.324977, batch error rate 52.000000%\n",
      "At minibatch 2200, batch loss 1.498189, batch nll 0.944157, batch error rate 36.000000%\n",
      "At minibatch 2300, batch loss 1.789805, batch nll 1.270635, batch error rate 44.000000%\n",
      "At minibatch 2400, batch loss 1.697296, batch nll 1.203872, batch error rate 40.000000%\n",
      "At minibatch 2500, batch loss 1.539178, batch nll 1.070189, batch error rate 32.000000%\n",
      "At minibatch 2600, batch loss 1.790241, batch nll 1.340317, batch error rate 56.000000%\n",
      "At minibatch 2700, batch loss 1.983976, batch nll 1.542800, batch error rate 68.000000%\n",
      "At minibatch 2800, batch loss 1.412843, batch nll 0.984076, batch error rate 36.000000%\n",
      "At minibatch 2900, batch loss 1.253244, batch nll 0.836117, batch error rate 20.000000%\n",
      "At minibatch 3000, batch loss 1.461072, batch nll 1.050966, batch error rate 40.000000%\n",
      "At minibatch 3100, batch loss 1.673890, batch nll 1.267666, batch error rate 36.000000%\n",
      "At minibatch 3200, batch loss 1.671789, batch nll 1.261383, batch error rate 40.000000%\n",
      "After epoch 2: valid_err_rate: 38.700000% currently going to do 4 epochs\n",
      "After epoch 2: averaged train_err_rate: 41.207500% averaged train nll: 1.160609 averaged train loss: 1.720645\n",
      "At minibatch 3300, batch loss 1.597917, batch nll 1.187735, batch error rate 36.000000%\n",
      "At minibatch 3400, batch loss 1.708171, batch nll 1.297451, batch error rate 48.000000%\n",
      "At minibatch 3500, batch loss 1.352891, batch nll 0.941463, batch error rate 32.000000%\n",
      "At minibatch 3600, batch loss 1.145138, batch nll 0.737885, batch error rate 24.000000%\n",
      "At minibatch 3700, batch loss 1.480201, batch nll 1.071834, batch error rate 36.000000%\n",
      "At minibatch 3800, batch loss 1.745011, batch nll 1.334069, batch error rate 52.000000%\n",
      "At minibatch 3900, batch loss 1.547119, batch nll 1.135664, batch error rate 48.000000%\n",
      "At minibatch 4000, batch loss 1.470684, batch nll 1.063471, batch error rate 32.000000%\n",
      "At minibatch 4100, batch loss 1.140341, batch nll 0.730223, batch error rate 24.000000%\n",
      "At minibatch 4200, batch loss 1.342290, batch nll 0.932754, batch error rate 40.000000%\n",
      "At minibatch 4300, batch loss 1.417667, batch nll 1.008466, batch error rate 32.000000%\n",
      "At minibatch 4400, batch loss 1.440822, batch nll 1.030785, batch error rate 44.000000%\n",
      "At minibatch 4500, batch loss 1.237687, batch nll 0.828617, batch error rate 20.000000%\n",
      "At minibatch 4600, batch loss 1.771130, batch nll 1.362262, batch error rate 48.000000%\n",
      "At minibatch 4700, batch loss 1.190169, batch nll 0.783155, batch error rate 28.000000%\n",
      "At minibatch 4800, batch loss 1.217789, batch nll 0.807890, batch error rate 20.000000%\n",
      "After epoch 3: valid_err_rate: 37.640000% currently going to do 5 epochs\n",
      "After epoch 3: averaged train_err_rate: 38.060000% averaged train nll: 1.082710 averaged train loss: 1.492127\n",
      "At minibatch 4900, batch loss 1.460074, batch nll 1.042257, batch error rate 40.000000%\n",
      "At minibatch 5000, batch loss 1.573857, batch nll 1.158496, batch error rate 52.000000%\n",
      "At minibatch 5100, batch loss 1.862210, batch nll 1.441629, batch error rate 44.000000%\n",
      "At minibatch 5200, batch loss 1.444859, batch nll 1.023502, batch error rate 36.000000%\n",
      "At minibatch 5300, batch loss 1.422415, batch nll 1.000421, batch error rate 40.000000%\n",
      "At minibatch 5400, batch loss 1.479946, batch nll 1.056556, batch error rate 36.000000%\n",
      "At minibatch 5500, batch loss 1.920775, batch nll 1.495346, batch error rate 48.000000%\n",
      "At minibatch 5600, batch loss 1.163003, batch nll 0.742439, batch error rate 28.000000%\n",
      "At minibatch 5700, batch loss 1.350075, batch nll 0.927843, batch error rate 28.000000%\n",
      "At minibatch 5800, batch loss 1.425338, batch nll 1.004507, batch error rate 40.000000%\n",
      "At minibatch 5900, batch loss 1.785514, batch nll 1.363971, batch error rate 44.000000%\n",
      "At minibatch 6000, batch loss 1.491424, batch nll 1.069844, batch error rate 28.000000%\n",
      "At minibatch 6100, batch loss 1.740172, batch nll 1.317324, batch error rate 64.000000%\n",
      "At minibatch 6200, batch loss 1.387462, batch nll 0.970130, batch error rate 40.000000%\n",
      "At minibatch 6300, batch loss 1.445835, batch nll 1.027098, batch error rate 40.000000%\n",
      "At minibatch 6400, batch loss 1.938445, batch nll 1.518079, batch error rate 48.000000%\n",
      "After epoch 4: valid_err_rate: 37.300000% currently going to do 7 epochs\n",
      "After epoch 4: averaged train_err_rate: 36.762500% averaged train nll: 1.047037 averaged train loss: 1.467831\n",
      "At minibatch 6500, batch loss 1.601751, batch nll 1.170623, batch error rate 40.000000%\n",
      "At minibatch 6600, batch loss 1.242079, batch nll 0.810624, batch error rate 36.000000%\n",
      "At minibatch 6700, batch loss 1.373832, batch nll 0.938729, batch error rate 40.000000%\n",
      "At minibatch 6800, batch loss 1.432179, batch nll 0.990630, batch error rate 32.000000%\n",
      "At minibatch 6900, batch loss 1.498519, batch nll 1.064121, batch error rate 44.000000%\n",
      "At minibatch 7000, batch loss 1.192528, batch nll 0.755543, batch error rate 24.000000%\n",
      "At minibatch 7100, batch loss 1.502397, batch nll 1.065761, batch error rate 36.000000%\n",
      "At minibatch 7200, batch loss 1.164460, batch nll 0.726034, batch error rate 28.000000%\n",
      "At minibatch 7300, batch loss 2.001329, batch nll 1.565488, batch error rate 52.000000%\n",
      "At minibatch 7400, batch loss 1.844811, batch nll 1.406718, batch error rate 40.000000%\n",
      "At minibatch 7500, batch loss 1.371904, batch nll 0.939758, batch error rate 28.000000%\n",
      "At minibatch 7600, batch loss 1.576266, batch nll 1.149668, batch error rate 44.000000%\n",
      "At minibatch 7700, batch loss 1.697395, batch nll 1.266274, batch error rate 44.000000%\n",
      "At minibatch 7800, batch loss 1.781774, batch nll 1.354312, batch error rate 56.000000%\n",
      "At minibatch 7900, batch loss 1.033559, batch nll 0.603316, batch error rate 24.000000%\n",
      "At minibatch 8000, batch loss 1.300557, batch nll 0.867617, batch error rate 24.000000%\n",
      "After epoch 5: valid_err_rate: 36.740000% currently going to do 8 epochs\n",
      "After epoch 5: averaged train_err_rate: 36.585000% averaged train nll: 1.036577 averaged train loss: 1.470213\n",
      "At minibatch 8100, batch loss 1.527093, batch nll 1.087001, batch error rate 48.000000%\n",
      "At minibatch 8200, batch loss 1.525427, batch nll 1.083084, batch error rate 28.000000%\n",
      "At minibatch 8300, batch loss 1.316689, batch nll 0.874768, batch error rate 32.000000%\n",
      "At minibatch 8400, batch loss 1.305690, batch nll 0.860005, batch error rate 20.000000%\n",
      "At minibatch 8500, batch loss 1.733108, batch nll 1.288596, batch error rate 56.000000%\n",
      "At minibatch 8600, batch loss 1.560938, batch nll 1.118324, batch error rate 28.000000%\n",
      "At minibatch 8700, batch loss 1.291376, batch nll 0.849442, batch error rate 36.000000%\n",
      "At minibatch 8800, batch loss 1.228398, batch nll 0.784728, batch error rate 24.000000%\n",
      "At minibatch 8900, batch loss 1.399544, batch nll 0.954406, batch error rate 36.000000%\n",
      "At minibatch 9000, batch loss 1.557802, batch nll 1.118598, batch error rate 48.000000%\n",
      "At minibatch 9100, batch loss 1.346567, batch nll 0.906906, batch error rate 28.000000%\n",
      "At minibatch 9200, batch loss 1.244168, batch nll 0.806937, batch error rate 32.000000%\n",
      "At minibatch 9300, batch loss 1.438401, batch nll 1.000092, batch error rate 28.000000%\n",
      "At minibatch 9400, batch loss 1.480719, batch nll 1.044108, batch error rate 32.000000%\n",
      "At minibatch 9500, batch loss 1.487229, batch nll 1.053326, batch error rate 36.000000%\n",
      "At minibatch 9600, batch loss 1.267276, batch nll 0.832602, batch error rate 20.000000%\n",
      "After epoch 6: valid_err_rate: 38.420000% currently going to do 8 epochs\n",
      "After epoch 6: averaged train_err_rate: 35.477500% averaged train nll: 1.014265 averaged train loss: 1.454914\n",
      "At minibatch 9700, batch loss 1.406232, batch nll 0.967202, batch error rate 36.000000%\n",
      "At minibatch 9800, batch loss 1.373290, batch nll 0.929754, batch error rate 32.000000%\n",
      "At minibatch 9900, batch loss 1.265141, batch nll 0.817833, batch error rate 24.000000%\n",
      "At minibatch 10000, batch loss 1.099118, batch nll 0.654241, batch error rate 16.000000%\n",
      "At minibatch 10100, batch loss 1.405321, batch nll 0.956172, batch error rate 32.000000%\n",
      "At minibatch 10200, batch loss 1.564304, batch nll 1.113673, batch error rate 44.000000%\n",
      "At minibatch 10300, batch loss 1.499737, batch nll 1.050137, batch error rate 28.000000%\n",
      "At minibatch 10400, batch loss 1.599109, batch nll 1.150910, batch error rate 40.000000%\n",
      "At minibatch 10500, batch loss 1.267397, batch nll 0.822308, batch error rate 24.000000%\n",
      "At minibatch 10600, batch loss 1.229267, batch nll 0.786309, batch error rate 28.000000%\n",
      "At minibatch 10700, batch loss 1.435672, batch nll 0.996093, batch error rate 32.000000%\n",
      "At minibatch 10800, batch loss 1.343001, batch nll 0.903150, batch error rate 28.000000%\n",
      "At minibatch 10900, batch loss 1.308369, batch nll 0.869203, batch error rate 24.000000%\n",
      "At minibatch 11000, batch loss 1.290451, batch nll 0.850726, batch error rate 28.000000%\n",
      "At minibatch 11100, batch loss 1.488912, batch nll 1.052812, batch error rate 36.000000%\n",
      "At minibatch 11200, batch loss 1.316740, batch nll 0.882757, batch error rate 28.000000%\n",
      "After epoch 7: valid_err_rate: 34.770000% currently going to do 11 epochs\n",
      "After epoch 7: averaged train_err_rate: 35.472500% averaged train nll: 1.010828 averaged train loss: 1.453897\n",
      "At minibatch 11300, batch loss 1.236430, batch nll 0.799827, batch error rate 28.000000%\n",
      "At minibatch 11400, batch loss 1.196584, batch nll 0.760140, batch error rate 32.000000%\n",
      "At minibatch 11500, batch loss 1.343488, batch nll 0.902296, batch error rate 36.000000%\n",
      "At minibatch 11600, batch loss 1.578938, batch nll 1.137627, batch error rate 40.000000%\n",
      "At minibatch 11700, batch loss 1.640315, batch nll 1.200842, batch error rate 40.000000%\n",
      "At minibatch 11800, batch loss 1.510482, batch nll 1.071461, batch error rate 48.000000%\n",
      "At minibatch 11900, batch loss 1.449769, batch nll 1.005355, batch error rate 36.000000%\n",
      "At minibatch 12000, batch loss 1.540344, batch nll 1.101085, batch error rate 40.000000%\n",
      "At minibatch 12100, batch loss 1.191379, batch nll 0.755296, batch error rate 20.000000%\n",
      "At minibatch 12200, batch loss 1.568117, batch nll 1.131398, batch error rate 52.000000%\n",
      "At minibatch 12300, batch loss 1.658533, batch nll 1.219644, batch error rate 56.000000%\n",
      "At minibatch 12400, batch loss 1.296772, batch nll 0.862742, batch error rate 24.000000%\n",
      "At minibatch 12500, batch loss 1.332848, batch nll 0.901924, batch error rate 36.000000%\n",
      "At minibatch 12600, batch loss 1.619636, batch nll 1.191882, batch error rate 40.000000%\n",
      "At minibatch 12700, batch loss 1.165482, batch nll 0.737267, batch error rate 24.000000%\n",
      "At minibatch 12800, batch loss 1.521707, batch nll 1.095931, batch error rate 36.000000%\n",
      "After epoch 8: valid_err_rate: 33.250000% currently going to do 13 epochs\n",
      "After epoch 8: averaged train_err_rate: 33.852500% averaged train nll: 0.966838 averaged train loss: 1.402884\n",
      "At minibatch 12900, batch loss 1.296524, batch nll 0.866207, batch error rate 32.000000%\n",
      "At minibatch 13000, batch loss 0.912399, batch nll 0.475497, batch error rate 12.000000%\n",
      "At minibatch 13100, batch loss 1.249016, batch nll 0.812654, batch error rate 20.000000%\n",
      "At minibatch 13200, batch loss 1.189235, batch nll 0.750643, batch error rate 24.000000%\n",
      "At minibatch 13300, batch loss 1.852446, batch nll 1.413284, batch error rate 56.000000%\n",
      "At minibatch 13400, batch loss 0.971195, batch nll 0.534671, batch error rate 16.000000%\n",
      "At minibatch 13500, batch loss 1.594605, batch nll 1.158137, batch error rate 44.000000%\n",
      "At minibatch 13600, batch loss 1.140771, batch nll 0.708174, batch error rate 28.000000%\n",
      "At minibatch 13700, batch loss 1.201276, batch nll 0.768041, batch error rate 40.000000%\n",
      "At minibatch 13800, batch loss 1.241047, batch nll 0.808573, batch error rate 32.000000%\n",
      "At minibatch 13900, batch loss 1.331882, batch nll 0.899505, batch error rate 28.000000%\n",
      "At minibatch 14000, batch loss 1.496196, batch nll 1.065423, batch error rate 40.000000%\n",
      "At minibatch 14100, batch loss 1.628856, batch nll 1.202166, batch error rate 44.000000%\n",
      "At minibatch 14200, batch loss 1.235837, batch nll 0.809324, batch error rate 28.000000%\n",
      "At minibatch 14300, batch loss 1.260185, batch nll 0.832744, batch error rate 32.000000%\n",
      "At minibatch 14400, batch loss 1.230505, batch nll 0.805240, batch error rate 24.000000%\n",
      "After epoch 9: valid_err_rate: 32.520000% currently going to do 14 epochs\n",
      "After epoch 9: averaged train_err_rate: 32.525000% averaged train nll: 0.930643 averaged train loss: 1.363223\n",
      "At minibatch 14500, batch loss 1.329237, batch nll 0.900081, batch error rate 44.000000%\n",
      "At minibatch 14600, batch loss 1.108048, batch nll 0.674050, batch error rate 16.000000%\n",
      "At minibatch 14700, batch loss 1.136470, batch nll 0.704043, batch error rate 24.000000%\n",
      "At minibatch 14800, batch loss 1.499659, batch nll 1.068504, batch error rate 36.000000%\n",
      "At minibatch 14900, batch loss 1.178356, batch nll 0.746258, batch error rate 28.000000%\n",
      "At minibatch 15000, batch loss 1.321261, batch nll 0.888997, batch error rate 28.000000%\n",
      "At minibatch 15100, batch loss 1.514403, batch nll 1.080866, batch error rate 44.000000%\n",
      "At minibatch 15200, batch loss 1.787934, batch nll 1.353189, batch error rate 40.000000%\n",
      "At minibatch 15300, batch loss 1.505759, batch nll 1.072676, batch error rate 40.000000%\n",
      "At minibatch 15400, batch loss 1.378665, batch nll 0.945050, batch error rate 24.000000%\n",
      "At minibatch 15500, batch loss 1.096681, batch nll 0.664949, batch error rate 28.000000%\n",
      "At minibatch 15600, batch loss 1.172635, batch nll 0.740071, batch error rate 32.000000%\n",
      "At minibatch 15700, batch loss 1.381324, batch nll 0.954052, batch error rate 44.000000%\n",
      "At minibatch 15800, batch loss 1.082491, batch nll 0.657357, batch error rate 20.000000%\n",
      "At minibatch 15900, batch loss 1.078063, batch nll 0.654126, batch error rate 16.000000%\n",
      "At minibatch 16000, batch loss 1.288995, batch nll 0.861205, batch error rate 32.000000%\n",
      "After epoch 10: valid_err_rate: 37.350000% currently going to do 14 epochs\n",
      "After epoch 10: averaged train_err_rate: 31.400000% averaged train nll: 0.902399 averaged train loss: 1.332942\n",
      "At minibatch 16100, batch loss 1.321117, batch nll 0.892244, batch error rate 28.000000%\n",
      "At minibatch 16200, batch loss 1.554327, batch nll 1.118966, batch error rate 36.000000%\n",
      "At minibatch 16300, batch loss 1.557474, batch nll 1.120326, batch error rate 32.000000%\n",
      "At minibatch 16400, batch loss 1.309803, batch nll 0.872000, batch error rate 36.000000%\n",
      "At minibatch 16500, batch loss 1.364681, batch nll 0.930467, batch error rate 28.000000%\n",
      "At minibatch 16600, batch loss 1.380604, batch nll 0.946333, batch error rate 32.000000%\n",
      "At minibatch 16700, batch loss 1.063916, batch nll 0.630818, batch error rate 28.000000%\n",
      "At minibatch 16800, batch loss 1.399777, batch nll 0.967110, batch error rate 36.000000%\n",
      "At minibatch 16900, batch loss 1.208585, batch nll 0.777729, batch error rate 28.000000%\n",
      "At minibatch 17000, batch loss 1.433681, batch nll 1.003790, batch error rate 40.000000%\n",
      "At minibatch 17100, batch loss 2.123680, batch nll 1.697492, batch error rate 44.000000%\n",
      "At minibatch 17200, batch loss 1.558486, batch nll 1.131239, batch error rate 40.000000%\n",
      "At minibatch 17300, batch loss 1.220663, batch nll 0.795487, batch error rate 28.000000%\n",
      "At minibatch 17400, batch loss 1.569227, batch nll 1.145781, batch error rate 32.000000%\n",
      "At minibatch 17500, batch loss 1.107023, batch nll 0.684450, batch error rate 20.000000%\n",
      "At minibatch 17600, batch loss 1.344280, batch nll 0.923631, batch error rate 28.000000%\n",
      "After epoch 11: valid_err_rate: 30.950000% currently going to do 17 epochs\n",
      "After epoch 11: averaged train_err_rate: 30.217500% averaged train nll: 0.873726 averaged train loss: 1.303907\n",
      "At minibatch 17700, batch loss 1.501218, batch nll 1.076230, batch error rate 36.000000%\n",
      "At minibatch 17800, batch loss 1.204287, batch nll 0.776578, batch error rate 28.000000%\n",
      "At minibatch 17900, batch loss 1.295589, batch nll 0.865287, batch error rate 32.000000%\n",
      "At minibatch 18000, batch loss 1.302093, batch nll 0.870449, batch error rate 32.000000%\n",
      "At minibatch 18100, batch loss 1.184287, batch nll 0.752580, batch error rate 28.000000%\n",
      "At minibatch 18200, batch loss 1.109370, batch nll 0.677515, batch error rate 28.000000%\n",
      "At minibatch 18300, batch loss 1.691864, batch nll 1.261340, batch error rate 56.000000%\n",
      "At minibatch 18400, batch loss 1.259645, batch nll 0.826182, batch error rate 28.000000%\n",
      "At minibatch 18500, batch loss 1.360082, batch nll 0.923873, batch error rate 40.000000%\n",
      "At minibatch 18600, batch loss 1.203829, batch nll 0.769958, batch error rate 28.000000%\n",
      "At minibatch 18700, batch loss 1.032427, batch nll 0.601664, batch error rate 12.000000%\n",
      "At minibatch 18800, batch loss 1.302945, batch nll 0.872636, batch error rate 40.000000%\n",
      "At minibatch 18900, batch loss 0.930017, batch nll 0.501683, batch error rate 12.000000%\n",
      "At minibatch 19000, batch loss 1.638510, batch nll 1.211030, batch error rate 56.000000%\n",
      "At minibatch 19100, batch loss 1.241713, batch nll 0.815983, batch error rate 20.000000%\n",
      "At minibatch 19200, batch loss 1.471285, batch nll 1.047758, batch error rate 40.000000%\n",
      "After epoch 12: valid_err_rate: 31.820000% currently going to do 17 epochs\n",
      "After epoch 12: averaged train_err_rate: 29.387500% averaged train nll: 0.844589 averaged train loss: 1.274756\n",
      "At minibatch 19300, batch loss 1.145236, batch nll 0.716556, batch error rate 20.000000%\n",
      "At minibatch 19400, batch loss 1.252262, batch nll 0.821410, batch error rate 36.000000%\n",
      "At minibatch 19500, batch loss 1.348375, batch nll 0.916085, batch error rate 40.000000%\n",
      "At minibatch 19600, batch loss 1.448419, batch nll 1.014864, batch error rate 36.000000%\n",
      "At minibatch 19700, batch loss 1.205897, batch nll 0.771177, batch error rate 28.000000%\n",
      "At minibatch 19800, batch loss 1.784726, batch nll 1.349382, batch error rate 48.000000%\n",
      "At minibatch 19900, batch loss 1.455068, batch nll 1.018817, batch error rate 28.000000%\n",
      "At minibatch 20000, batch loss 1.368392, batch nll 0.933835, batch error rate 36.000000%\n",
      "At minibatch 20100, batch loss 1.320786, batch nll 0.886063, batch error rate 28.000000%\n",
      "At minibatch 20200, batch loss 1.405142, batch nll 0.971045, batch error rate 36.000000%\n",
      "At minibatch 20300, batch loss 1.421965, batch nll 0.989658, batch error rate 40.000000%\n",
      "At minibatch 20400, batch loss 1.401298, batch nll 0.967269, batch error rate 40.000000%\n",
      "At minibatch 20500, batch loss 0.983378, batch nll 0.551033, batch error rate 16.000000%\n",
      "At minibatch 20600, batch loss 1.296610, batch nll 0.865762, batch error rate 32.000000%\n",
      "At minibatch 20700, batch loss 1.478070, batch nll 1.050715, batch error rate 32.000000%\n",
      "At minibatch 20800, batch loss 1.195931, batch nll 0.768923, batch error rate 32.000000%\n",
      "After epoch 13: valid_err_rate: 29.140000% currently going to do 20 epochs\n",
      "After epoch 13: averaged train_err_rate: 28.572500% averaged train nll: 0.821924 averaged train loss: 1.254322\n",
      "At minibatch 20900, batch loss 1.204432, batch nll 0.771925, batch error rate 24.000000%\n",
      "At minibatch 21000, batch loss 1.106483, batch nll 0.671411, batch error rate 28.000000%\n",
      "At minibatch 21100, batch loss 1.223690, batch nll 0.787925, batch error rate 24.000000%\n",
      "At minibatch 21200, batch loss 1.489939, batch nll 1.053595, batch error rate 44.000000%\n",
      "At minibatch 21300, batch loss 1.297681, batch nll 0.863094, batch error rate 32.000000%\n",
      "At minibatch 21400, batch loss 1.230192, batch nll 0.794850, batch error rate 28.000000%\n",
      "At minibatch 21500, batch loss 1.368717, batch nll 0.930800, batch error rate 32.000000%\n",
      "At minibatch 21600, batch loss 1.243297, batch nll 0.806950, batch error rate 32.000000%\n",
      "At minibatch 21700, batch loss 1.047447, batch nll 0.612470, batch error rate 20.000000%\n",
      "At minibatch 21800, batch loss 0.946470, batch nll 0.514959, batch error rate 20.000000%\n",
      "At minibatch 21900, batch loss 1.101933, batch nll 0.669133, batch error rate 24.000000%\n",
      "At minibatch 22000, batch loss 1.128128, batch nll 0.694155, batch error rate 20.000000%\n",
      "At minibatch 22100, batch loss 1.271423, batch nll 0.838135, batch error rate 32.000000%\n",
      "At minibatch 22200, batch loss 1.009895, batch nll 0.578962, batch error rate 20.000000%\n",
      "At minibatch 22300, batch loss 1.327938, batch nll 0.898173, batch error rate 32.000000%\n",
      "At minibatch 22400, batch loss 1.190597, batch nll 0.762260, batch error rate 32.000000%\n",
      "After epoch 14: valid_err_rate: 30.180000% currently going to do 20 epochs\n",
      "After epoch 14: averaged train_err_rate: 27.862500% averaged train nll: 0.800754 averaged train loss: 1.234409\n",
      "At minibatch 22500, batch loss 1.031937, batch nll 0.600309, batch error rate 20.000000%\n",
      "At minibatch 22600, batch loss 1.096614, batch nll 0.663130, batch error rate 36.000000%\n",
      "At minibatch 22700, batch loss 0.873658, batch nll 0.437657, batch error rate 20.000000%\n",
      "At minibatch 22800, batch loss 0.945970, batch nll 0.511636, batch error rate 12.000000%\n",
      "At minibatch 22900, batch loss 0.989342, batch nll 0.556803, batch error rate 20.000000%\n",
      "At minibatch 23000, batch loss 1.186877, batch nll 0.752971, batch error rate 28.000000%\n",
      "At minibatch 23100, batch loss 1.184031, batch nll 0.750902, batch error rate 32.000000%\n",
      "At minibatch 23200, batch loss 1.295932, batch nll 0.861700, batch error rate 36.000000%\n",
      "At minibatch 23300, batch loss 1.128456, batch nll 0.691578, batch error rate 24.000000%\n",
      "At minibatch 23400, batch loss 1.081967, batch nll 0.645478, batch error rate 24.000000%\n",
      "At minibatch 23500, batch loss 1.131407, batch nll 0.695422, batch error rate 32.000000%\n",
      "At minibatch 23600, batch loss 0.929798, batch nll 0.494337, batch error rate 12.000000%\n",
      "At minibatch 23700, batch loss 1.468312, batch nll 1.034722, batch error rate 32.000000%\n",
      "At minibatch 23800, batch loss 1.007446, batch nll 0.575631, batch error rate 20.000000%\n",
      "At minibatch 23900, batch loss 0.995550, batch nll 0.564758, batch error rate 20.000000%\n",
      "At minibatch 24000, batch loss 1.191709, batch nll 0.762657, batch error rate 28.000000%\n",
      "After epoch 15: valid_err_rate: 31.380000% currently going to do 20 epochs\n",
      "After epoch 15: averaged train_err_rate: 26.917500% averaged train nll: 0.782366 averaged train loss: 1.216108\n",
      "At minibatch 24100, batch loss 1.174933, batch nll 0.742704, batch error rate 32.000000%\n",
      "At minibatch 24200, batch loss 1.228644, batch nll 0.794671, batch error rate 36.000000%\n",
      "At minibatch 24300, batch loss 1.212586, batch nll 0.776713, batch error rate 20.000000%\n",
      "At minibatch 24400, batch loss 1.181449, batch nll 0.742865, batch error rate 24.000000%\n",
      "At minibatch 24500, batch loss 1.243961, batch nll 0.803405, batch error rate 20.000000%\n",
      "At minibatch 24600, batch loss 1.451182, batch nll 1.011016, batch error rate 36.000000%\n",
      "At minibatch 24700, batch loss 1.282013, batch nll 0.842092, batch error rate 24.000000%\n",
      "At minibatch 24800, batch loss 0.886111, batch nll 0.448864, batch error rate 12.000000%\n",
      "At minibatch 24900, batch loss 1.174335, batch nll 0.738158, batch error rate 24.000000%\n",
      "At minibatch 25000, batch loss 1.261879, batch nll 0.827043, batch error rate 20.000000%\n",
      "At minibatch 25100, batch loss 1.133656, batch nll 0.697937, batch error rate 24.000000%\n",
      "At minibatch 25200, batch loss 1.061457, batch nll 0.625713, batch error rate 20.000000%\n",
      "At minibatch 25300, batch loss 1.403111, batch nll 0.968372, batch error rate 40.000000%\n",
      "At minibatch 25400, batch loss 0.982803, batch nll 0.549042, batch error rate 16.000000%\n",
      "At minibatch 25500, batch loss 1.111760, batch nll 0.678910, batch error rate 16.000000%\n",
      "At minibatch 25600, batch loss 1.021756, batch nll 0.589915, batch error rate 12.000000%\n",
      "After epoch 16: valid_err_rate: 28.720000% currently going to do 25 epochs\n",
      "After epoch 16: averaged train_err_rate: 26.312500% averaged train nll: 0.765966 averaged train loss: 1.201727\n",
      "At minibatch 25700, batch loss 1.098306, batch nll 0.661324, batch error rate 20.000000%\n",
      "At minibatch 25800, batch loss 1.386038, batch nll 0.947063, batch error rate 36.000000%\n",
      "At minibatch 25900, batch loss 1.318375, batch nll 0.878358, batch error rate 32.000000%\n",
      "At minibatch 26000, batch loss 1.051344, batch nll 0.609215, batch error rate 20.000000%\n",
      "At minibatch 26100, batch loss 1.442400, batch nll 0.998679, batch error rate 28.000000%\n",
      "At minibatch 26200, batch loss 1.060456, batch nll 0.616238, batch error rate 20.000000%\n",
      "At minibatch 26300, batch loss 1.575030, batch nll 1.130740, batch error rate 32.000000%\n",
      "At minibatch 26400, batch loss 1.150242, batch nll 0.705855, batch error rate 28.000000%\n",
      "At minibatch 26500, batch loss 0.883277, batch nll 0.442506, batch error rate 4.000000%\n",
      "At minibatch 26600, batch loss 1.053630, batch nll 0.612910, batch error rate 20.000000%\n",
      "At minibatch 26700, batch loss 1.190952, batch nll 0.750632, batch error rate 24.000000%\n",
      "At minibatch 26800, batch loss 1.263656, batch nll 0.825547, batch error rate 36.000000%\n",
      "At minibatch 26900, batch loss 1.150404, batch nll 0.710961, batch error rate 24.000000%\n",
      "At minibatch 27000, batch loss 1.201935, batch nll 0.762969, batch error rate 32.000000%\n",
      "At minibatch 27100, batch loss 1.283989, batch nll 0.845651, batch error rate 36.000000%\n",
      "At minibatch 27200, batch loss 1.199992, batch nll 0.763131, batch error rate 20.000000%\n",
      "After epoch 17: valid_err_rate: 27.680000% currently going to do 26 epochs\n",
      "After epoch 17: averaged train_err_rate: 25.737500% averaged train nll: 0.746094 averaged train loss: 1.186179\n",
      "At minibatch 27300, batch loss 1.032808, batch nll 0.592985, batch error rate 20.000000%\n",
      "At minibatch 27400, batch loss 1.277307, batch nll 0.836615, batch error rate 32.000000%\n",
      "At minibatch 27500, batch loss 1.006930, batch nll 0.564432, batch error rate 20.000000%\n",
      "At minibatch 27600, batch loss 1.272496, batch nll 0.828236, batch error rate 32.000000%\n",
      "At minibatch 27700, batch loss 1.353765, batch nll 0.909812, batch error rate 32.000000%\n",
      "At minibatch 27800, batch loss 0.958487, batch nll 0.514695, batch error rate 20.000000%\n",
      "At minibatch 27900, batch loss 1.145592, batch nll 0.701990, batch error rate 16.000000%\n",
      "At minibatch 28000, batch loss 1.224111, batch nll 0.781123, batch error rate 32.000000%\n",
      "At minibatch 28100, batch loss 1.095853, batch nll 0.650523, batch error rate 20.000000%\n",
      "At minibatch 28200, batch loss 0.880918, batch nll 0.437562, batch error rate 8.000000%\n",
      "At minibatch 28300, batch loss 1.229708, batch nll 0.785908, batch error rate 32.000000%\n",
      "At minibatch 28400, batch loss 1.083786, batch nll 0.641256, batch error rate 24.000000%\n",
      "At minibatch 28500, batch loss 1.588289, batch nll 1.148137, batch error rate 40.000000%\n",
      "At minibatch 28600, batch loss 1.107491, batch nll 0.666495, batch error rate 28.000000%\n",
      "At minibatch 28700, batch loss 1.538420, batch nll 1.099056, batch error rate 40.000000%\n",
      "At minibatch 28800, batch loss 1.347605, batch nll 0.911137, batch error rate 28.000000%\n",
      "After epoch 18: valid_err_rate: 28.080000% currently going to do 26 epochs\n",
      "After epoch 18: averaged train_err_rate: 25.302500% averaged train nll: 0.730330 averaged train loss: 1.172634\n",
      "At minibatch 28900, batch loss 1.494662, batch nll 1.053021, batch error rate 44.000000%\n",
      "At minibatch 29000, batch loss 1.107929, batch nll 0.662899, batch error rate 20.000000%\n",
      "At minibatch 29100, batch loss 0.990742, batch nll 0.545193, batch error rate 16.000000%\n",
      "At minibatch 29200, batch loss 1.229844, batch nll 0.782280, batch error rate 32.000000%\n",
      "At minibatch 29300, batch loss 1.004765, batch nll 0.556987, batch error rate 24.000000%\n",
      "At minibatch 29400, batch loss 1.223105, batch nll 0.773648, batch error rate 40.000000%\n",
      "At minibatch 29500, batch loss 0.902577, batch nll 0.453587, batch error rate 12.000000%\n",
      "At minibatch 29600, batch loss 1.168201, batch nll 0.719746, batch error rate 20.000000%\n",
      "At minibatch 29700, batch loss 1.524257, batch nll 1.075814, batch error rate 32.000000%\n",
      "At minibatch 29800, batch loss 1.074426, batch nll 0.627577, batch error rate 32.000000%\n",
      "At minibatch 29900, batch loss 1.098082, batch nll 0.651036, batch error rate 24.000000%\n",
      "At minibatch 30000, batch loss 1.088282, batch nll 0.642522, batch error rate 20.000000%\n",
      "At minibatch 30100, batch loss 1.154987, batch nll 0.712033, batch error rate 16.000000%\n",
      "At minibatch 30200, batch loss 1.632546, batch nll 1.190427, batch error rate 36.000000%\n",
      "At minibatch 30300, batch loss 1.539050, batch nll 1.097379, batch error rate 40.000000%\n",
      "At minibatch 30400, batch loss 0.907875, batch nll 0.468309, batch error rate 16.000000%\n",
      "After epoch 19: valid_err_rate: 29.590000% currently going to do 26 epochs\n",
      "After epoch 19: averaged train_err_rate: 24.905000% averaged train nll: 0.718595 averaged train loss: 1.164164\n",
      "At minibatch 30500, batch loss 0.931420, batch nll 0.488809, batch error rate 16.000000%\n",
      "At minibatch 30600, batch loss 1.277789, batch nll 0.832000, batch error rate 28.000000%\n",
      "At minibatch 30700, batch loss 1.149095, batch nll 0.702290, batch error rate 20.000000%\n",
      "At minibatch 30800, batch loss 1.494525, batch nll 1.046049, batch error rate 36.000000%\n",
      "At minibatch 30900, batch loss 1.262018, batch nll 0.812411, batch error rate 28.000000%\n",
      "At minibatch 31000, batch loss 1.165355, batch nll 0.715913, batch error rate 16.000000%\n",
      "At minibatch 31100, batch loss 1.278534, batch nll 0.828883, batch error rate 40.000000%\n",
      "At minibatch 31200, batch loss 0.913429, batch nll 0.464709, batch error rate 16.000000%\n",
      "At minibatch 31300, batch loss 1.138340, batch nll 0.690202, batch error rate 24.000000%\n",
      "At minibatch 31400, batch loss 1.437239, batch nll 0.989567, batch error rate 24.000000%\n",
      "At minibatch 31500, batch loss 1.068748, batch nll 0.622374, batch error rate 24.000000%\n",
      "At minibatch 31600, batch loss 1.180208, batch nll 0.731652, batch error rate 24.000000%\n",
      "At minibatch 31700, batch loss 1.268049, batch nll 0.821548, batch error rate 16.000000%\n",
      "At minibatch 31800, batch loss 1.123677, batch nll 0.677159, batch error rate 20.000000%\n",
      "At minibatch 31900, batch loss 1.327209, batch nll 0.880316, batch error rate 28.000000%\n",
      "At minibatch 32000, batch loss 1.492869, batch nll 1.047587, batch error rate 36.000000%\n",
      "After epoch 20: valid_err_rate: 28.120000% currently going to do 26 epochs\n",
      "After epoch 20: averaged train_err_rate: 24.167500% averaged train nll: 0.706226 averaged train loss: 1.153403\n",
      "At minibatch 32100, batch loss 0.943146, batch nll 0.494645, batch error rate 24.000000%\n",
      "At minibatch 32200, batch loss 1.044218, batch nll 0.593646, batch error rate 24.000000%\n",
      "At minibatch 32300, batch loss 1.277657, batch nll 0.825984, batch error rate 36.000000%\n",
      "At minibatch 32400, batch loss 0.928702, batch nll 0.476043, batch error rate 8.000000%\n",
      "At minibatch 32500, batch loss 1.221832, batch nll 0.769300, batch error rate 24.000000%\n",
      "At minibatch 32600, batch loss 1.117321, batch nll 0.666096, batch error rate 16.000000%\n",
      "At minibatch 32700, batch loss 1.204696, batch nll 0.753037, batch error rate 28.000000%\n",
      "At minibatch 32800, batch loss 1.219871, batch nll 0.767610, batch error rate 24.000000%\n",
      "At minibatch 32900, batch loss 1.628783, batch nll 1.177471, batch error rate 40.000000%\n",
      "At minibatch 33000, batch loss 1.038598, batch nll 0.587252, batch error rate 16.000000%\n",
      "At minibatch 33100, batch loss 1.217016, batch nll 0.765887, batch error rate 20.000000%\n",
      "At minibatch 33200, batch loss 1.181259, batch nll 0.731561, batch error rate 24.000000%\n",
      "At minibatch 33300, batch loss 1.311913, batch nll 0.863239, batch error rate 24.000000%\n",
      "At minibatch 33400, batch loss 1.270904, batch nll 0.822855, batch error rate 24.000000%\n",
      "At minibatch 33500, batch loss 1.335181, batch nll 0.888262, batch error rate 24.000000%\n",
      "At minibatch 33600, batch loss 0.818401, batch nll 0.371995, batch error rate 16.000000%\n",
      "After epoch 21: valid_err_rate: 26.960000% currently going to do 32 epochs\n",
      "After epoch 21: averaged train_err_rate: 23.640000% averaged train nll: 0.689366 averaged train loss: 1.139771\n",
      "At minibatch 33700, batch loss 1.322595, batch nll 0.872127, batch error rate 20.000000%\n",
      "At minibatch 33800, batch loss 0.967906, batch nll 0.514755, batch error rate 16.000000%\n",
      "At minibatch 33900, batch loss 1.269461, batch nll 0.815978, batch error rate 16.000000%\n",
      "At minibatch 34000, batch loss 0.748041, batch nll 0.292758, batch error rate 8.000000%\n",
      "At minibatch 34100, batch loss 0.998196, batch nll 0.542605, batch error rate 24.000000%\n",
      "At minibatch 34200, batch loss 0.752786, batch nll 0.296187, batch error rate 8.000000%\n",
      "At minibatch 34300, batch loss 1.279083, batch nll 0.823007, batch error rate 32.000000%\n",
      "At minibatch 34400, batch loss 1.188177, batch nll 0.732942, batch error rate 28.000000%\n",
      "At minibatch 34500, batch loss 1.196730, batch nll 0.740960, batch error rate 32.000000%\n",
      "At minibatch 34600, batch loss 1.102207, batch nll 0.646675, batch error rate 20.000000%\n",
      "At minibatch 34700, batch loss 1.155016, batch nll 0.699424, batch error rate 20.000000%\n",
      "At minibatch 34800, batch loss 1.309349, batch nll 0.853867, batch error rate 28.000000%\n",
      "At minibatch 34900, batch loss 1.270442, batch nll 0.816662, batch error rate 20.000000%\n",
      "At minibatch 35000, batch loss 1.043363, batch nll 0.589452, batch error rate 24.000000%\n",
      "At minibatch 35100, batch loss 1.589667, batch nll 1.138770, batch error rate 40.000000%\n",
      "At minibatch 35200, batch loss 0.950906, batch nll 0.500363, batch error rate 16.000000%\n",
      "After epoch 22: valid_err_rate: 26.300000% currently going to do 34 epochs\n",
      "After epoch 22: averaged train_err_rate: 23.392500% averaged train nll: 0.681624 averaged train loss: 1.135675\n",
      "At minibatch 35300, batch loss 0.971748, batch nll 0.518541, batch error rate 20.000000%\n",
      "At minibatch 35400, batch loss 0.939477, batch nll 0.485374, batch error rate 12.000000%\n",
      "At minibatch 35500, batch loss 0.890417, batch nll 0.433936, batch error rate 8.000000%\n",
      "At minibatch 35600, batch loss 1.150340, batch nll 0.692630, batch error rate 28.000000%\n",
      "At minibatch 35700, batch loss 0.912089, batch nll 0.454037, batch error rate 12.000000%\n",
      "At minibatch 35800, batch loss 1.219404, batch nll 0.761835, batch error rate 28.000000%\n",
      "At minibatch 35900, batch loss 1.294555, batch nll 0.835781, batch error rate 36.000000%\n",
      "At minibatch 36000, batch loss 1.400501, batch nll 0.942116, batch error rate 32.000000%\n",
      "At minibatch 36100, batch loss 1.147768, batch nll 0.689139, batch error rate 28.000000%\n",
      "At minibatch 36200, batch loss 1.218813, batch nll 0.759690, batch error rate 28.000000%\n",
      "At minibatch 36300, batch loss 1.342973, batch nll 0.885339, batch error rate 32.000000%\n",
      "At minibatch 36400, batch loss 1.205313, batch nll 0.747495, batch error rate 24.000000%\n",
      "At minibatch 36500, batch loss 1.363854, batch nll 0.907169, batch error rate 32.000000%\n",
      "At minibatch 36600, batch loss 1.100312, batch nll 0.643802, batch error rate 20.000000%\n",
      "At minibatch 36700, batch loss 1.012732, batch nll 0.557780, batch error rate 20.000000%\n",
      "At minibatch 36800, batch loss 1.092802, batch nll 0.637848, batch error rate 28.000000%\n",
      "After epoch 23: valid_err_rate: 27.370000% currently going to do 34 epochs\n",
      "After epoch 23: averaged train_err_rate: 22.997500% averaged train nll: 0.668864 averaged train loss: 1.125547\n",
      "At minibatch 36900, batch loss 1.024661, batch nll 0.566646, batch error rate 16.000000%\n",
      "At minibatch 37000, batch loss 1.086333, batch nll 0.626859, batch error rate 20.000000%\n",
      "At minibatch 37100, batch loss 0.902402, batch nll 0.441566, batch error rate 16.000000%\n",
      "At minibatch 37200, batch loss 0.984247, batch nll 0.522008, batch error rate 20.000000%\n",
      "At minibatch 37300, batch loss 1.158745, batch nll 0.696055, batch error rate 28.000000%\n",
      "At minibatch 37400, batch loss 1.276799, batch nll 0.814321, batch error rate 32.000000%\n",
      "At minibatch 37500, batch loss 0.887996, batch nll 0.425299, batch error rate 12.000000%\n",
      "At minibatch 37600, batch loss 1.023641, batch nll 0.561089, batch error rate 28.000000%\n",
      "At minibatch 37700, batch loss 1.152227, batch nll 0.689843, batch error rate 40.000000%\n",
      "At minibatch 37800, batch loss 0.938097, batch nll 0.477083, batch error rate 12.000000%\n",
      "At minibatch 37900, batch loss 1.276667, batch nll 0.815501, batch error rate 24.000000%\n",
      "At minibatch 38000, batch loss 0.955885, batch nll 0.495810, batch error rate 16.000000%\n",
      "At minibatch 38100, batch loss 1.010628, batch nll 0.550860, batch error rate 20.000000%\n",
      "At minibatch 38200, batch loss 1.120360, batch nll 0.661895, batch error rate 24.000000%\n",
      "At minibatch 38300, batch loss 1.158916, batch nll 0.701744, batch error rate 20.000000%\n",
      "At minibatch 38400, batch loss 1.050606, batch nll 0.594109, batch error rate 16.000000%\n",
      "After epoch 24: valid_err_rate: 26.740000% currently going to do 34 epochs\n",
      "After epoch 24: averaged train_err_rate: 22.197500% averaged train nll: 0.651276 averaged train loss: 1.111564\n",
      "At minibatch 38500, batch loss 1.153542, batch nll 0.694052, batch error rate 28.000000%\n",
      "At minibatch 38600, batch loss 1.379536, batch nll 0.917850, batch error rate 32.000000%\n",
      "At minibatch 38700, batch loss 0.861976, batch nll 0.398555, batch error rate 12.000000%\n",
      "At minibatch 38800, batch loss 1.360964, batch nll 0.897499, batch error rate 28.000000%\n",
      "At minibatch 38900, batch loss 1.757023, batch nll 1.292790, batch error rate 40.000000%\n",
      "At minibatch 39000, batch loss 1.092775, batch nll 0.627616, batch error rate 24.000000%\n",
      "At minibatch 39100, batch loss 1.175394, batch nll 0.708925, batch error rate 28.000000%\n",
      "At minibatch 39200, batch loss 1.069586, batch nll 0.603222, batch error rate 24.000000%\n",
      "At minibatch 39300, batch loss 1.403214, batch nll 0.938545, batch error rate 40.000000%\n",
      "At minibatch 39400, batch loss 0.992935, batch nll 0.528427, batch error rate 20.000000%\n",
      "At minibatch 39500, batch loss 0.862692, batch nll 0.399023, batch error rate 12.000000%\n",
      "At minibatch 39600, batch loss 1.488859, batch nll 1.026686, batch error rate 44.000000%\n",
      "At minibatch 39700, batch loss 1.249887, batch nll 0.786865, batch error rate 20.000000%\n",
      "At minibatch 39800, batch loss 1.413500, batch nll 0.950946, batch error rate 32.000000%\n",
      "At minibatch 39900, batch loss 1.204605, batch nll 0.744029, batch error rate 36.000000%\n",
      "At minibatch 40000, batch loss 1.112402, batch nll 0.653110, batch error rate 16.000000%\n",
      "After epoch 25: valid_err_rate: 27.270000% currently going to do 34 epochs\n",
      "After epoch 25: averaged train_err_rate: 21.570000% averaged train nll: 0.636942 averaged train loss: 1.100001\n",
      "At minibatch 40100, batch loss 1.127892, batch nll 0.665305, batch error rate 24.000000%\n",
      "At minibatch 40200, batch loss 1.029650, batch nll 0.565829, batch error rate 16.000000%\n",
      "At minibatch 40300, batch loss 1.191836, batch nll 0.726224, batch error rate 24.000000%\n",
      "At minibatch 40400, batch loss 1.137584, batch nll 0.669658, batch error rate 20.000000%\n",
      "At minibatch 40500, batch loss 1.399763, batch nll 0.932367, batch error rate 24.000000%\n",
      "At minibatch 40600, batch loss 0.842339, batch nll 0.375239, batch error rate 8.000000%\n",
      "At minibatch 40700, batch loss 1.149638, batch nll 0.682999, batch error rate 16.000000%\n",
      "At minibatch 40800, batch loss 1.119978, batch nll 0.653216, batch error rate 28.000000%\n",
      "At minibatch 40900, batch loss 0.849262, batch nll 0.381602, batch error rate 8.000000%\n",
      "At minibatch 41000, batch loss 1.207166, batch nll 0.740368, batch error rate 32.000000%\n",
      "At minibatch 41100, batch loss 1.328212, batch nll 0.861763, batch error rate 40.000000%\n",
      "At minibatch 41200, batch loss 1.086198, batch nll 0.620287, batch error rate 20.000000%\n",
      "At minibatch 41300, batch loss 1.015962, batch nll 0.549873, batch error rate 16.000000%\n",
      "At minibatch 41400, batch loss 1.050178, batch nll 0.584587, batch error rate 24.000000%\n",
      "At minibatch 41500, batch loss 0.877606, batch nll 0.411808, batch error rate 12.000000%\n",
      "At minibatch 41600, batch loss 1.312938, batch nll 0.848562, batch error rate 32.000000%\n",
      "After epoch 26: valid_err_rate: 25.920000% currently going to do 40 epochs\n",
      "After epoch 26: averaged train_err_rate: 21.435000% averaged train nll: 0.632972 averaged train loss: 1.098943\n",
      "At minibatch 41700, batch loss 1.182310, batch nll 0.715169, batch error rate 32.000000%\n",
      "At minibatch 41800, batch loss 1.461120, batch nll 0.992198, batch error rate 40.000000%\n",
      "At minibatch 41900, batch loss 1.127625, batch nll 0.656764, batch error rate 24.000000%\n",
      "At minibatch 42000, batch loss 1.036224, batch nll 0.565856, batch error rate 20.000000%\n",
      "At minibatch 42100, batch loss 1.125891, batch nll 0.655820, batch error rate 20.000000%\n",
      "At minibatch 42200, batch loss 1.215906, batch nll 0.745800, batch error rate 24.000000%\n",
      "At minibatch 42300, batch loss 1.129264, batch nll 0.659587, batch error rate 16.000000%\n",
      "At minibatch 42400, batch loss 1.114001, batch nll 0.643835, batch error rate 20.000000%\n",
      "At minibatch 42500, batch loss 1.161522, batch nll 0.690844, batch error rate 32.000000%\n",
      "At minibatch 42600, batch loss 1.170333, batch nll 0.699537, batch error rate 24.000000%\n",
      "At minibatch 42700, batch loss 1.143623, batch nll 0.672786, batch error rate 24.000000%\n",
      "At minibatch 42800, batch loss 0.981201, batch nll 0.511170, batch error rate 12.000000%\n",
      "At minibatch 42900, batch loss 1.425318, batch nll 0.956070, batch error rate 32.000000%\n",
      "At minibatch 43000, batch loss 0.934065, batch nll 0.465272, batch error rate 20.000000%\n",
      "At minibatch 43100, batch loss 1.098922, batch nll 0.629875, batch error rate 20.000000%\n",
      "At minibatch 43200, batch loss 1.215168, batch nll 0.747452, batch error rate 28.000000%\n",
      "After epoch 27: valid_err_rate: 26.050000% currently going to do 40 epochs\n",
      "After epoch 27: averaged train_err_rate: 21.092500% averaged train nll: 0.621171 averaged train loss: 1.090793\n",
      "At minibatch 43300, batch loss 0.891408, batch nll 0.421653, batch error rate 12.000000%\n",
      "At minibatch 43400, batch loss 0.992312, batch nll 0.520824, batch error rate 16.000000%\n",
      "At minibatch 43500, batch loss 1.254976, batch nll 0.782054, batch error rate 24.000000%\n",
      "At minibatch 43600, batch loss 1.267600, batch nll 0.794328, batch error rate 24.000000%\n",
      "At minibatch 43700, batch loss 1.138098, batch nll 0.664373, batch error rate 28.000000%\n",
      "At minibatch 43800, batch loss 1.206428, batch nll 0.732623, batch error rate 24.000000%\n",
      "At minibatch 43900, batch loss 1.157377, batch nll 0.683404, batch error rate 28.000000%\n",
      "At minibatch 44000, batch loss 0.955357, batch nll 0.482216, batch error rate 16.000000%\n",
      "At minibatch 44100, batch loss 1.228921, batch nll 0.755805, batch error rate 24.000000%\n",
      "At minibatch 44200, batch loss 1.179852, batch nll 0.706397, batch error rate 24.000000%\n",
      "At minibatch 44300, batch loss 1.045936, batch nll 0.573056, batch error rate 24.000000%\n",
      "At minibatch 44400, batch loss 1.079825, batch nll 0.607150, batch error rate 20.000000%\n",
      "At minibatch 44500, batch loss 1.277438, batch nll 0.806375, batch error rate 20.000000%\n",
      "At minibatch 44600, batch loss 1.314746, batch nll 0.843879, batch error rate 40.000000%\n",
      "At minibatch 44700, batch loss 1.253654, batch nll 0.784215, batch error rate 40.000000%\n",
      "At minibatch 44800, batch loss 1.222688, batch nll 0.753762, batch error rate 28.000000%\n",
      "After epoch 28: valid_err_rate: 26.120000% currently going to do 40 epochs\n",
      "After epoch 28: averaged train_err_rate: 20.692500% averaged train nll: 0.611130 averaged train loss: 1.083309\n",
      "At minibatch 44900, batch loss 1.027052, batch nll 0.555459, batch error rate 16.000000%\n",
      "At minibatch 45000, batch loss 1.121150, batch nll 0.647931, batch error rate 20.000000%\n",
      "At minibatch 45100, batch loss 1.264468, batch nll 0.789033, batch error rate 32.000000%\n",
      "At minibatch 45200, batch loss 0.991128, batch nll 0.515602, batch error rate 16.000000%\n",
      "At minibatch 45300, batch loss 1.006075, batch nll 0.529557, batch error rate 20.000000%\n",
      "At minibatch 45400, batch loss 0.953605, batch nll 0.477225, batch error rate 8.000000%\n",
      "At minibatch 45500, batch loss 1.138984, batch nll 0.661322, batch error rate 16.000000%\n",
      "At minibatch 45600, batch loss 1.073835, batch nll 0.595991, batch error rate 12.000000%\n",
      "At minibatch 45700, batch loss 1.263795, batch nll 0.786573, batch error rate 32.000000%\n",
      "At minibatch 45800, batch loss 1.032268, batch nll 0.555083, batch error rate 12.000000%\n",
      "At minibatch 45900, batch loss 1.199423, batch nll 0.723220, batch error rate 20.000000%\n",
      "At minibatch 46000, batch loss 1.037037, batch nll 0.560526, batch error rate 20.000000%\n",
      "At minibatch 46100, batch loss 1.130814, batch nll 0.653989, batch error rate 28.000000%\n",
      "At minibatch 46200, batch loss 0.953518, batch nll 0.477946, batch error rate 24.000000%\n",
      "At minibatch 46300, batch loss 0.856710, batch nll 0.383113, batch error rate 12.000000%\n",
      "At minibatch 46400, batch loss 1.115113, batch nll 0.643331, batch error rate 32.000000%\n",
      "After epoch 29: valid_err_rate: 25.880000% currently going to do 44 epochs\n",
      "After epoch 29: averaged train_err_rate: 20.190000% averaged train nll: 0.597513 averaged train loss: 1.073080\n",
      "At minibatch 46500, batch loss 0.818267, batch nll 0.343079, batch error rate 4.000000%\n",
      "At minibatch 46600, batch loss 1.137460, batch nll 0.660730, batch error rate 24.000000%\n",
      "At minibatch 46700, batch loss 1.024473, batch nll 0.545552, batch error rate 20.000000%\n",
      "At minibatch 46800, batch loss 1.085688, batch nll 0.606423, batch error rate 24.000000%\n",
      "At minibatch 46900, batch loss 0.918203, batch nll 0.439035, batch error rate 12.000000%\n",
      "At minibatch 47000, batch loss 1.177621, batch nll 0.698431, batch error rate 20.000000%\n",
      "At minibatch 47100, batch loss 1.608950, batch nll 1.129690, batch error rate 40.000000%\n",
      "At minibatch 47200, batch loss 1.484467, batch nll 1.003704, batch error rate 40.000000%\n",
      "At minibatch 47300, batch loss 1.104079, batch nll 0.623282, batch error rate 16.000000%\n",
      "At minibatch 47400, batch loss 0.924961, batch nll 0.444081, batch error rate 12.000000%\n",
      "At minibatch 47500, batch loss 1.115623, batch nll 0.635511, batch error rate 24.000000%\n",
      "At minibatch 47600, batch loss 1.113394, batch nll 0.632911, batch error rate 20.000000%\n",
      "At minibatch 47700, batch loss 0.974642, batch nll 0.495734, batch error rate 16.000000%\n",
      "At minibatch 47800, batch loss 0.989772, batch nll 0.512418, batch error rate 20.000000%\n",
      "At minibatch 47900, batch loss 1.240471, batch nll 0.762087, batch error rate 16.000000%\n",
      "At minibatch 48000, batch loss 1.097046, batch nll 0.619454, batch error rate 24.000000%\n",
      "After epoch 30: valid_err_rate: 26.940000% currently going to do 44 epochs\n",
      "After epoch 30: averaged train_err_rate: 20.100000% averaged train nll: 0.590258 averaged train loss: 1.069025\n",
      "At minibatch 48100, batch loss 1.006600, batch nll 0.526051, batch error rate 16.000000%\n",
      "At minibatch 48200, batch loss 1.041903, batch nll 0.559298, batch error rate 20.000000%\n",
      "At minibatch 48300, batch loss 1.063604, batch nll 0.579585, batch error rate 16.000000%\n",
      "At minibatch 48400, batch loss 1.106680, batch nll 0.621608, batch error rate 28.000000%\n",
      "At minibatch 48500, batch loss 1.223725, batch nll 0.737834, batch error rate 28.000000%\n",
      "At minibatch 48600, batch loss 1.132832, batch nll 0.646263, batch error rate 20.000000%\n",
      "At minibatch 48700, batch loss 0.874141, batch nll 0.387390, batch error rate 16.000000%\n",
      "At minibatch 48800, batch loss 0.834417, batch nll 0.347664, batch error rate 12.000000%\n",
      "At minibatch 48900, batch loss 1.060678, batch nll 0.574411, batch error rate 24.000000%\n",
      "At minibatch 49000, batch loss 1.020701, batch nll 0.535184, batch error rate 16.000000%\n",
      "At minibatch 49100, batch loss 1.131111, batch nll 0.646475, batch error rate 28.000000%\n",
      "At minibatch 49200, batch loss 1.223890, batch nll 0.739830, batch error rate 32.000000%\n",
      "At minibatch 49300, batch loss 1.105506, batch nll 0.621675, batch error rate 28.000000%\n",
      "At minibatch 49400, batch loss 1.061639, batch nll 0.578673, batch error rate 24.000000%\n",
      "At minibatch 49500, batch loss 0.986584, batch nll 0.505101, batch error rate 8.000000%\n",
      "At minibatch 49600, batch loss 1.168829, batch nll 0.687482, batch error rate 32.000000%\n",
      "After epoch 31: valid_err_rate: 25.790000% currently going to do 47 epochs\n",
      "After epoch 31: averaged train_err_rate: 19.712500% averaged train nll: 0.580400 averaged train loss: 1.064518\n",
      "At minibatch 49700, batch loss 1.125363, batch nll 0.643116, batch error rate 16.000000%\n",
      "At minibatch 49800, batch loss 1.026467, batch nll 0.542078, batch error rate 12.000000%\n",
      "At minibatch 49900, batch loss 0.945580, batch nll 0.459217, batch error rate 12.000000%\n",
      "At minibatch 50000, batch loss 1.082671, batch nll 0.595939, batch error rate 28.000000%\n",
      "At minibatch 50100, batch loss 1.012854, batch nll 0.526204, batch error rate 24.000000%\n",
      "At minibatch 50200, batch loss 0.975391, batch nll 0.487423, batch error rate 16.000000%\n",
      "At minibatch 50300, batch loss 0.822943, batch nll 0.335735, batch error rate 8.000000%\n",
      "At minibatch 50400, batch loss 1.107926, batch nll 0.621003, batch error rate 16.000000%\n",
      "At minibatch 50500, batch loss 1.022678, batch nll 0.535246, batch error rate 16.000000%\n",
      "At minibatch 50600, batch loss 1.076306, batch nll 0.589463, batch error rate 20.000000%\n",
      "At minibatch 50700, batch loss 0.980616, batch nll 0.494650, batch error rate 12.000000%\n",
      "At minibatch 50800, batch loss 1.198316, batch nll 0.712693, batch error rate 32.000000%\n",
      "At minibatch 50900, batch loss 1.125490, batch nll 0.639468, batch error rate 28.000000%\n",
      "At minibatch 51000, batch loss 1.108606, batch nll 0.622239, batch error rate 28.000000%\n",
      "At minibatch 51100, batch loss 1.642121, batch nll 1.155050, batch error rate 44.000000%\n",
      "At minibatch 51200, batch loss 1.012125, batch nll 0.526086, batch error rate 12.000000%\n",
      "After epoch 32: valid_err_rate: 25.190000% currently going to do 49 epochs\n",
      "After epoch 32: averaged train_err_rate: 19.240000% averaged train nll: 0.570655 averaged train loss: 1.056741\n",
      "At minibatch 51300, batch loss 1.146815, batch nll 0.658561, batch error rate 32.000000%\n",
      "At minibatch 51400, batch loss 1.159486, batch nll 0.669918, batch error rate 24.000000%\n",
      "At minibatch 51500, batch loss 0.994297, batch nll 0.503330, batch error rate 8.000000%\n",
      "At minibatch 51600, batch loss 1.140167, batch nll 0.648758, batch error rate 24.000000%\n",
      "At minibatch 51700, batch loss 1.087818, batch nll 0.596142, batch error rate 16.000000%\n",
      "At minibatch 51800, batch loss 0.911060, batch nll 0.419864, batch error rate 16.000000%\n",
      "At minibatch 51900, batch loss 1.046971, batch nll 0.556140, batch error rate 16.000000%\n",
      "At minibatch 52000, batch loss 0.896643, batch nll 0.404970, batch error rate 16.000000%\n",
      "At minibatch 52100, batch loss 0.759293, batch nll 0.267645, batch error rate 4.000000%\n",
      "At minibatch 52200, batch loss 1.020136, batch nll 0.528146, batch error rate 12.000000%\n",
      "At minibatch 52300, batch loss 0.824743, batch nll 0.333377, batch error rate 12.000000%\n",
      "At minibatch 52400, batch loss 1.183031, batch nll 0.691966, batch error rate 28.000000%\n",
      "At minibatch 52500, batch loss 1.409155, batch nll 0.919561, batch error rate 40.000000%\n",
      "At minibatch 52600, batch loss 1.044065, batch nll 0.554274, batch error rate 24.000000%\n",
      "At minibatch 52700, batch loss 1.327147, batch nll 0.837145, batch error rate 28.000000%\n",
      "At minibatch 52800, batch loss 1.041869, batch nll 0.552984, batch error rate 20.000000%\n",
      "After epoch 33: valid_err_rate: 25.530000% currently going to do 49 epochs\n",
      "After epoch 33: averaged train_err_rate: 18.925000% averaged train nll: 0.563646 averaged train loss: 1.054217\n",
      "At minibatch 52900, batch loss 0.997410, batch nll 0.505554, batch error rate 12.000000%\n",
      "At minibatch 53000, batch loss 0.830698, batch nll 0.337939, batch error rate 4.000000%\n",
      "At minibatch 53100, batch loss 1.023360, batch nll 0.530283, batch error rate 16.000000%\n",
      "At minibatch 53200, batch loss 1.039801, batch nll 0.546321, batch error rate 12.000000%\n",
      "At minibatch 53300, batch loss 0.874861, batch nll 0.381406, batch error rate 12.000000%\n",
      "At minibatch 53400, batch loss 0.860715, batch nll 0.367466, batch error rate 8.000000%\n",
      "At minibatch 53500, batch loss 1.012897, batch nll 0.518785, batch error rate 8.000000%\n",
      "At minibatch 53600, batch loss 0.930572, batch nll 0.436397, batch error rate 12.000000%\n",
      "At minibatch 53700, batch loss 1.039140, batch nll 0.544547, batch error rate 24.000000%\n",
      "At minibatch 53800, batch loss 1.116241, batch nll 0.623353, batch error rate 28.000000%\n",
      "At minibatch 53900, batch loss 1.173062, batch nll 0.680177, batch error rate 16.000000%\n",
      "At minibatch 54000, batch loss 0.991851, batch nll 0.498316, batch error rate 20.000000%\n",
      "At minibatch 54100, batch loss 1.117961, batch nll 0.624540, batch error rate 20.000000%\n",
      "At minibatch 54200, batch loss 0.880642, batch nll 0.387526, batch error rate 16.000000%\n",
      "At minibatch 54300, batch loss 1.157533, batch nll 0.665161, batch error rate 24.000000%\n",
      "At minibatch 54400, batch loss 1.472812, batch nll 0.981179, batch error rate 36.000000%\n",
      "After epoch 34: valid_err_rate: 25.900000% currently going to do 49 epochs\n",
      "After epoch 34: averaged train_err_rate: 18.407500% averaged train nll: 0.549355 averaged train loss: 1.042418\n",
      "At minibatch 54500, batch loss 0.969585, batch nll 0.476074, batch error rate 20.000000%\n",
      "At minibatch 54600, batch loss 1.310491, batch nll 0.815097, batch error rate 36.000000%\n",
      "At minibatch 54700, batch loss 0.929353, batch nll 0.433089, batch error rate 8.000000%\n",
      "At minibatch 54800, batch loss 1.224259, batch nll 0.726449, batch error rate 32.000000%\n",
      "At minibatch 54900, batch loss 0.799091, batch nll 0.301123, batch error rate 8.000000%\n",
      "At minibatch 55000, batch loss 1.055446, batch nll 0.557236, batch error rate 20.000000%\n",
      "At minibatch 55100, batch loss 0.902689, batch nll 0.404598, batch error rate 12.000000%\n",
      "At minibatch 55200, batch loss 1.010910, batch nll 0.512686, batch error rate 16.000000%\n",
      "At minibatch 55300, batch loss 1.282519, batch nll 0.784163, batch error rate 24.000000%\n",
      "At minibatch 55400, batch loss 1.194723, batch nll 0.696793, batch error rate 32.000000%\n",
      "At minibatch 55500, batch loss 1.195859, batch nll 0.698687, batch error rate 20.000000%\n",
      "At minibatch 55600, batch loss 1.149867, batch nll 0.653137, batch error rate 28.000000%\n",
      "At minibatch 55700, batch loss 0.871236, batch nll 0.375443, batch error rate 8.000000%\n",
      "At minibatch 55800, batch loss 1.026138, batch nll 0.530699, batch error rate 24.000000%\n",
      "At minibatch 55900, batch loss 1.156474, batch nll 0.661597, batch error rate 16.000000%\n",
      "At minibatch 56000, batch loss 1.045275, batch nll 0.551359, batch error rate 28.000000%\n",
      "After epoch 35: valid_err_rate: 24.730000% currently going to do 53 epochs\n",
      "After epoch 35: averaged train_err_rate: 18.055000% averaged train nll: 0.541648 averaged train loss: 1.038246\n",
      "At minibatch 56100, batch loss 1.164201, batch nll 0.668215, batch error rate 24.000000%\n",
      "At minibatch 56200, batch loss 1.225304, batch nll 0.728116, batch error rate 32.000000%\n",
      "At minibatch 56300, batch loss 1.124376, batch nll 0.626271, batch error rate 16.000000%\n",
      "At minibatch 56400, batch loss 1.080132, batch nll 0.581119, batch error rate 20.000000%\n",
      "At minibatch 56500, batch loss 0.826669, batch nll 0.326333, batch error rate 12.000000%\n",
      "At minibatch 56600, batch loss 0.722072, batch nll 0.221822, batch error rate 8.000000%\n",
      "At minibatch 56700, batch loss 1.114775, batch nll 0.614506, batch error rate 16.000000%\n",
      "At minibatch 56800, batch loss 0.970643, batch nll 0.469608, batch error rate 16.000000%\n",
      "At minibatch 56900, batch loss 1.226766, batch nll 0.726341, batch error rate 28.000000%\n",
      "At minibatch 57000, batch loss 1.214899, batch nll 0.714939, batch error rate 16.000000%\n",
      "At minibatch 57100, batch loss 1.025440, batch nll 0.526216, batch error rate 16.000000%\n",
      "At minibatch 57200, batch loss 0.959698, batch nll 0.461076, batch error rate 16.000000%\n",
      "At minibatch 57300, batch loss 1.228784, batch nll 0.730554, batch error rate 20.000000%\n",
      "At minibatch 57400, batch loss 1.281984, batch nll 0.784718, batch error rate 16.000000%\n",
      "At minibatch 57500, batch loss 1.144328, batch nll 0.647010, batch error rate 20.000000%\n",
      "At minibatch 57600, batch loss 0.853575, batch nll 0.356655, batch error rate 12.000000%\n",
      "After epoch 36: valid_err_rate: 25.200000% currently going to do 53 epochs\n",
      "After epoch 36: averaged train_err_rate: 17.780000% averaged train nll: 0.537651 averaged train loss: 1.036323\n",
      "At minibatch 57700, batch loss 1.119316, batch nll 0.620184, batch error rate 24.000000%\n",
      "At minibatch 57800, batch loss 0.924572, batch nll 0.424000, batch error rate 8.000000%\n",
      "At minibatch 57900, batch loss 0.919518, batch nll 0.417353, batch error rate 8.000000%\n",
      "At minibatch 58000, batch loss 0.947075, batch nll 0.444492, batch error rate 8.000000%\n",
      "At minibatch 58100, batch loss 0.979706, batch nll 0.476285, batch error rate 16.000000%\n",
      "At minibatch 58200, batch loss 0.643608, batch nll 0.139340, batch error rate 0.000000%\n",
      "At minibatch 58300, batch loss 1.068880, batch nll 0.563903, batch error rate 12.000000%\n",
      "At minibatch 58400, batch loss 0.880092, batch nll 0.375308, batch error rate 12.000000%\n",
      "At minibatch 58500, batch loss 1.375131, batch nll 0.871445, batch error rate 44.000000%\n",
      "At minibatch 58600, batch loss 1.090577, batch nll 0.586168, batch error rate 16.000000%\n",
      "At minibatch 58700, batch loss 1.330326, batch nll 0.826970, batch error rate 32.000000%\n",
      "At minibatch 58800, batch loss 1.025804, batch nll 0.522307, batch error rate 20.000000%\n",
      "At minibatch 58900, batch loss 1.007410, batch nll 0.504559, batch error rate 24.000000%\n",
      "At minibatch 59000, batch loss 0.874520, batch nll 0.372178, batch error rate 8.000000%\n",
      "At minibatch 59100, batch loss 1.109759, batch nll 0.607559, batch error rate 12.000000%\n",
      "At minibatch 59200, batch loss 0.855947, batch nll 0.353997, batch error rate 8.000000%\n",
      "After epoch 37: valid_err_rate: 24.640000% currently going to do 56 epochs\n",
      "After epoch 37: averaged train_err_rate: 17.515000% averaged train nll: 0.526442 averaged train loss: 1.029162\n",
      "At minibatch 59300, batch loss 1.012197, batch nll 0.509186, batch error rate 16.000000%\n",
      "At minibatch 59400, batch loss 0.935560, batch nll 0.432198, batch error rate 12.000000%\n",
      "At minibatch 59500, batch loss 0.915019, batch nll 0.409984, batch error rate 20.000000%\n",
      "At minibatch 59600, batch loss 1.060507, batch nll 0.554896, batch error rate 24.000000%\n",
      "At minibatch 59700, batch loss 0.979294, batch nll 0.472781, batch error rate 16.000000%\n",
      "At minibatch 59800, batch loss 0.864226, batch nll 0.357794, batch error rate 16.000000%\n",
      "At minibatch 59900, batch loss 1.070398, batch nll 0.563051, batch error rate 24.000000%\n",
      "At minibatch 60000, batch loss 1.016088, batch nll 0.508851, batch error rate 20.000000%\n",
      "At minibatch 60100, batch loss 1.137211, batch nll 0.629875, batch error rate 16.000000%\n",
      "At minibatch 60200, batch loss 0.809648, batch nll 0.302245, batch error rate 8.000000%\n",
      "At minibatch 60300, batch loss 1.002111, batch nll 0.494993, batch error rate 20.000000%\n",
      "At minibatch 60400, batch loss 1.263172, batch nll 0.755360, batch error rate 32.000000%\n",
      "At minibatch 60500, batch loss 0.816176, batch nll 0.308511, batch error rate 4.000000%\n",
      "At minibatch 60600, batch loss 1.213307, batch nll 0.707218, batch error rate 28.000000%\n",
      "At minibatch 60700, batch loss 0.777789, batch nll 0.271967, batch error rate 8.000000%\n",
      "At minibatch 60800, batch loss 1.024738, batch nll 0.519177, batch error rate 20.000000%\n",
      "After epoch 38: valid_err_rate: 24.680000% currently going to do 56 epochs\n",
      "After epoch 38: averaged train_err_rate: 16.982500% averaged train nll: 0.516321 averaged train loss: 1.022411\n",
      "At minibatch 60900, batch loss 1.095723, batch nll 0.587991, batch error rate 20.000000%\n",
      "At minibatch 61000, batch loss 1.247039, batch nll 0.737899, batch error rate 24.000000%\n",
      "At minibatch 61100, batch loss 0.921209, batch nll 0.411251, batch error rate 8.000000%\n",
      "At minibatch 61200, batch loss 1.057005, batch nll 0.546321, batch error rate 16.000000%\n",
      "At minibatch 61300, batch loss 0.811566, batch nll 0.300856, batch error rate 4.000000%\n",
      "At minibatch 61400, batch loss 0.910875, batch nll 0.400246, batch error rate 16.000000%\n",
      "At minibatch 61500, batch loss 1.392195, batch nll 0.881012, batch error rate 32.000000%\n",
      "At minibatch 61600, batch loss 0.820294, batch nll 0.308657, batch error rate 12.000000%\n",
      "At minibatch 61700, batch loss 1.175360, batch nll 0.662838, batch error rate 16.000000%\n",
      "At minibatch 61800, batch loss 1.021134, batch nll 0.509015, batch error rate 20.000000%\n",
      "At minibatch 61900, batch loss 0.916653, batch nll 0.405327, batch error rate 8.000000%\n",
      "At minibatch 62000, batch loss 1.297501, batch nll 0.787299, batch error rate 36.000000%\n",
      "At minibatch 62100, batch loss 1.027329, batch nll 0.517496, batch error rate 16.000000%\n",
      "At minibatch 62200, batch loss 1.036536, batch nll 0.527072, batch error rate 28.000000%\n",
      "At minibatch 62300, batch loss 1.069889, batch nll 0.560887, batch error rate 24.000000%\n",
      "At minibatch 62400, batch loss 0.859053, batch nll 0.350655, batch error rate 12.000000%\n",
      "After epoch 39: valid_err_rate: 24.210000% currently going to do 59 epochs\n",
      "After epoch 39: averaged train_err_rate: 16.650000% averaged train nll: 0.506354 averaged train loss: 1.016543\n",
      "At minibatch 62500, batch loss 0.889229, batch nll 0.379750, batch error rate 16.000000%\n",
      "At minibatch 62600, batch loss 1.420257, batch nll 0.909355, batch error rate 28.000000%\n",
      "At minibatch 62700, batch loss 1.007404, batch nll 0.495565, batch error rate 24.000000%\n",
      "At minibatch 62800, batch loss 0.922569, batch nll 0.409710, batch error rate 20.000000%\n",
      "At minibatch 62900, batch loss 1.155300, batch nll 0.641977, batch error rate 16.000000%\n",
      "At minibatch 63000, batch loss 1.082839, batch nll 0.569495, batch error rate 20.000000%\n",
      "At minibatch 63100, batch loss 0.943378, batch nll 0.429318, batch error rate 8.000000%\n",
      "At minibatch 63200, batch loss 0.985930, batch nll 0.472039, batch error rate 20.000000%\n",
      "At minibatch 63300, batch loss 0.932259, batch nll 0.418566, batch error rate 12.000000%\n",
      "At minibatch 63400, batch loss 1.162155, batch nll 0.648124, batch error rate 20.000000%\n",
      "At minibatch 63500, batch loss 1.189664, batch nll 0.675770, batch error rate 12.000000%\n",
      "At minibatch 63600, batch loss 1.106054, batch nll 0.591989, batch error rate 20.000000%\n",
      "At minibatch 63700, batch loss 1.170849, batch nll 0.657228, batch error rate 24.000000%\n",
      "At minibatch 63800, batch loss 0.937469, batch nll 0.424323, batch error rate 16.000000%\n",
      "At minibatch 63900, batch loss 1.179192, batch nll 0.666950, batch error rate 24.000000%\n",
      "At minibatch 64000, batch loss 0.985183, batch nll 0.473365, batch error rate 12.000000%\n",
      "After epoch 40: valid_err_rate: 25.150000% currently going to do 59 epochs\n",
      "After epoch 40: averaged train_err_rate: 16.717500% averaged train nll: 0.504893 averaged train loss: 1.017717\n",
      "At minibatch 64100, batch loss 0.868524, batch nll 0.355792, batch error rate 8.000000%\n",
      "At minibatch 64200, batch loss 1.068051, batch nll 0.553778, batch error rate 20.000000%\n",
      "At minibatch 64300, batch loss 0.848103, batch nll 0.332975, batch error rate 16.000000%\n",
      "At minibatch 64400, batch loss 0.886449, batch nll 0.371346, batch error rate 12.000000%\n",
      "At minibatch 64500, batch loss 0.925920, batch nll 0.409858, batch error rate 12.000000%\n",
      "At minibatch 64600, batch loss 0.829762, batch nll 0.312815, batch error rate 8.000000%\n",
      "At minibatch 64700, batch loss 0.934903, batch nll 0.417961, batch error rate 16.000000%\n",
      "At minibatch 64800, batch loss 1.048851, batch nll 0.531958, batch error rate 16.000000%\n",
      "At minibatch 64900, batch loss 0.919589, batch nll 0.403120, batch error rate 4.000000%\n",
      "At minibatch 65000, batch loss 1.088142, batch nll 0.570872, batch error rate 20.000000%\n",
      "At minibatch 65100, batch loss 1.194127, batch nll 0.676367, batch error rate 24.000000%\n",
      "At minibatch 65200, batch loss 0.823927, batch nll 0.306191, batch error rate 12.000000%\n",
      "At minibatch 65300, batch loss 0.928248, batch nll 0.411227, batch error rate 8.000000%\n",
      "At minibatch 65400, batch loss 1.011732, batch nll 0.495836, batch error rate 8.000000%\n",
      "At minibatch 65500, batch loss 0.865903, batch nll 0.349775, batch error rate 4.000000%\n",
      "At minibatch 65600, batch loss 1.241918, batch nll 0.725830, batch error rate 24.000000%\n",
      "After epoch 41: valid_err_rate: 25.160000% currently going to do 59 epochs\n",
      "After epoch 41: averaged train_err_rate: 15.980000% averaged train nll: 0.489077 averaged train loss: 1.005056\n",
      "At minibatch 65700, batch loss 0.846774, batch nll 0.328805, batch error rate 12.000000%\n",
      "At minibatch 65800, batch loss 0.981955, batch nll 0.463842, batch error rate 16.000000%\n",
      "At minibatch 65900, batch loss 0.770029, batch nll 0.251140, batch error rate 8.000000%\n",
      "At minibatch 66000, batch loss 1.165014, batch nll 0.645595, batch error rate 28.000000%\n",
      "At minibatch 66100, batch loss 1.137498, batch nll 0.617309, batch error rate 16.000000%\n",
      "At minibatch 66200, batch loss 0.997070, batch nll 0.476464, batch error rate 12.000000%\n",
      "At minibatch 66300, batch loss 1.257844, batch nll 0.737024, batch error rate 20.000000%\n",
      "At minibatch 66400, batch loss 1.048921, batch nll 0.527936, batch error rate 20.000000%\n",
      "At minibatch 66500, batch loss 1.013437, batch nll 0.491846, batch error rate 16.000000%\n",
      "At minibatch 66600, batch loss 1.067803, batch nll 0.546843, batch error rate 20.000000%\n",
      "At minibatch 66700, batch loss 1.157583, batch nll 0.637133, batch error rate 28.000000%\n",
      "At minibatch 66800, batch loss 1.089300, batch nll 0.568924, batch error rate 24.000000%\n",
      "At minibatch 66900, batch loss 1.026335, batch nll 0.506358, batch error rate 24.000000%\n",
      "At minibatch 67000, batch loss 0.889687, batch nll 0.370002, batch error rate 8.000000%\n",
      "At minibatch 67100, batch loss 1.106180, batch nll 0.587180, batch error rate 24.000000%\n",
      "At minibatch 67200, batch loss 0.951377, batch nll 0.433229, batch error rate 8.000000%\n",
      "After epoch 42: valid_err_rate: 24.180000% currently going to do 64 epochs\n",
      "After epoch 42: averaged train_err_rate: 15.812500% averaged train nll: 0.484734 averaged train loss: 1.004481\n",
      "At minibatch 67300, batch loss 0.934788, batch nll 0.415206, batch error rate 20.000000%\n",
      "At minibatch 67400, batch loss 1.427618, batch nll 0.907159, batch error rate 40.000000%\n",
      "At minibatch 67500, batch loss 0.897384, batch nll 0.376691, batch error rate 16.000000%\n",
      "At minibatch 67600, batch loss 1.069414, batch nll 0.548651, batch error rate 20.000000%\n",
      "At minibatch 67700, batch loss 1.074082, batch nll 0.552380, batch error rate 16.000000%\n",
      "At minibatch 67800, batch loss 1.121866, batch nll 0.599290, batch error rate 20.000000%\n",
      "At minibatch 67900, batch loss 0.821488, batch nll 0.298371, batch error rate 8.000000%\n",
      "At minibatch 68000, batch loss 1.042597, batch nll 0.519032, batch error rate 16.000000%\n",
      "At minibatch 68100, batch loss 1.031419, batch nll 0.508031, batch error rate 16.000000%\n",
      "At minibatch 68200, batch loss 0.879280, batch nll 0.356121, batch error rate 16.000000%\n",
      "At minibatch 68300, batch loss 1.115167, batch nll 0.592106, batch error rate 20.000000%\n",
      "At minibatch 68400, batch loss 0.901762, batch nll 0.379186, batch error rate 8.000000%\n",
      "At minibatch 68500, batch loss 1.006894, batch nll 0.484869, batch error rate 16.000000%\n",
      "At minibatch 68600, batch loss 0.986301, batch nll 0.464234, batch error rate 12.000000%\n",
      "At minibatch 68700, batch loss 1.172234, batch nll 0.650814, batch error rate 16.000000%\n",
      "At minibatch 68800, batch loss 1.135278, batch nll 0.614087, batch error rate 20.000000%\n",
      "After epoch 43: valid_err_rate: 24.440000% currently going to do 64 epochs\n",
      "After epoch 43: averaged train_err_rate: 15.190000% averaged train nll: 0.472123 averaged train loss: 0.993996\n",
      "At minibatch 68900, batch loss 1.216743, batch nll 0.693958, batch error rate 32.000000%\n",
      "At minibatch 69000, batch loss 0.872727, batch nll 0.349435, batch error rate 16.000000%\n",
      "At minibatch 69100, batch loss 0.838966, batch nll 0.315493, batch error rate 8.000000%\n",
      "At minibatch 69200, batch loss 1.045580, batch nll 0.521854, batch error rate 20.000000%\n",
      "At minibatch 69300, batch loss 0.996260, batch nll 0.471724, batch error rate 8.000000%\n",
      "At minibatch 69400, batch loss 0.706377, batch nll 0.180342, batch error rate 4.000000%\n",
      "At minibatch 69500, batch loss 0.761055, batch nll 0.235152, batch error rate 8.000000%\n",
      "At minibatch 69600, batch loss 0.960968, batch nll 0.434915, batch error rate 8.000000%\n",
      "At minibatch 69700, batch loss 1.067665, batch nll 0.541664, batch error rate 20.000000%\n",
      "At minibatch 69800, batch loss 0.917460, batch nll 0.391082, batch error rate 8.000000%\n",
      "At minibatch 69900, batch loss 0.859443, batch nll 0.333619, batch error rate 4.000000%\n",
      "At minibatch 70000, batch loss 1.068361, batch nll 0.542436, batch error rate 20.000000%\n",
      "At minibatch 70100, batch loss 1.076667, batch nll 0.551506, batch error rate 20.000000%\n",
      "At minibatch 70200, batch loss 0.736302, batch nll 0.211606, batch error rate 0.000000%\n",
      "At minibatch 70300, batch loss 0.992996, batch nll 0.468258, batch error rate 16.000000%\n",
      "At minibatch 70400, batch loss 0.792421, batch nll 0.268849, batch error rate 4.000000%\n",
      "After epoch 44: valid_err_rate: 24.210000% currently going to do 64 epochs\n",
      "After epoch 44: averaged train_err_rate: 15.172500% averaged train nll: 0.468411 averaged train loss: 0.993236\n",
      "At minibatch 70500, batch loss 0.928176, batch nll 0.402530, batch error rate 8.000000%\n",
      "At minibatch 70600, batch loss 1.011350, batch nll 0.484568, batch error rate 20.000000%\n",
      "At minibatch 70700, batch loss 0.952614, batch nll 0.424844, batch error rate 8.000000%\n",
      "At minibatch 70800, batch loss 1.257611, batch nll 0.729005, batch error rate 24.000000%\n",
      "At minibatch 70900, batch loss 0.785803, batch nll 0.257257, batch error rate 4.000000%\n",
      "At minibatch 71000, batch loss 0.813256, batch nll 0.284368, batch error rate 8.000000%\n",
      "At minibatch 71100, batch loss 1.071224, batch nll 0.542074, batch error rate 16.000000%\n",
      "At minibatch 71200, batch loss 1.103293, batch nll 0.573901, batch error rate 16.000000%\n",
      "At minibatch 71300, batch loss 1.116529, batch nll 0.587711, batch error rate 32.000000%\n",
      "At minibatch 71400, batch loss 0.852408, batch nll 0.323775, batch error rate 8.000000%\n",
      "At minibatch 71500, batch loss 1.049765, batch nll 0.521343, batch error rate 24.000000%\n",
      "At minibatch 71600, batch loss 0.846202, batch nll 0.318459, batch error rate 8.000000%\n",
      "At minibatch 71700, batch loss 1.197762, batch nll 0.670862, batch error rate 24.000000%\n",
      "At minibatch 71800, batch loss 0.985304, batch nll 0.458497, batch error rate 16.000000%\n",
      "At minibatch 71900, batch loss 0.850051, batch nll 0.323860, batch error rate 8.000000%\n",
      "At minibatch 72000, batch loss 0.970474, batch nll 0.444155, batch error rate 16.000000%\n",
      "After epoch 45: valid_err_rate: 23.420000% currently going to do 68 epochs\n",
      "After epoch 45: averaged train_err_rate: 14.882500% averaged train nll: 0.462984 averaged train loss: 0.990679\n",
      "At minibatch 72100, batch loss 0.931030, batch nll 0.403539, batch error rate 12.000000%\n",
      "At minibatch 72200, batch loss 0.824787, batch nll 0.295215, batch error rate 12.000000%\n",
      "At minibatch 72300, batch loss 0.861294, batch nll 0.331161, batch error rate 8.000000%\n",
      "At minibatch 72400, batch loss 1.034010, batch nll 0.503484, batch error rate 16.000000%\n",
      "At minibatch 72500, batch loss 1.112830, batch nll 0.581743, batch error rate 16.000000%\n",
      "At minibatch 72600, batch loss 0.774534, batch nll 0.243279, batch error rate 8.000000%\n",
      "At minibatch 72700, batch loss 0.881749, batch nll 0.350275, batch error rate 12.000000%\n",
      "At minibatch 72800, batch loss 1.313443, batch nll 0.782054, batch error rate 32.000000%\n",
      "At minibatch 72900, batch loss 0.964048, batch nll 0.432881, batch error rate 12.000000%\n",
      "At minibatch 73000, batch loss 1.034071, batch nll 0.502438, batch error rate 12.000000%\n",
      "At minibatch 73100, batch loss 0.780987, batch nll 0.249222, batch error rate 8.000000%\n",
      "At minibatch 73200, batch loss 0.759696, batch nll 0.228362, batch error rate 4.000000%\n",
      "At minibatch 73300, batch loss 0.982001, batch nll 0.451386, batch error rate 20.000000%\n",
      "At minibatch 73400, batch loss 1.012548, batch nll 0.481945, batch error rate 20.000000%\n",
      "At minibatch 73500, batch loss 1.013726, batch nll 0.484134, batch error rate 20.000000%\n",
      "At minibatch 73600, batch loss 0.987051, batch nll 0.457706, batch error rate 16.000000%\n",
      "After epoch 46: valid_err_rate: 24.860000% currently going to do 68 epochs\n",
      "After epoch 46: averaged train_err_rate: 14.402500% averaged train nll: 0.452615 averaged train loss: 0.983065\n",
      "At minibatch 73700, batch loss 1.034610, batch nll 0.504239, batch error rate 20.000000%\n",
      "At minibatch 73800, batch loss 0.826618, batch nll 0.295528, batch error rate 4.000000%\n",
      "At minibatch 73900, batch loss 1.318846, batch nll 0.787785, batch error rate 32.000000%\n",
      "At minibatch 74000, batch loss 0.994335, batch nll 0.462660, batch error rate 16.000000%\n",
      "At minibatch 74100, batch loss 0.985793, batch nll 0.453857, batch error rate 16.000000%\n",
      "At minibatch 74200, batch loss 1.019781, batch nll 0.487667, batch error rate 20.000000%\n",
      "At minibatch 74300, batch loss 0.773060, batch nll 0.241570, batch error rate 4.000000%\n",
      "At minibatch 74400, batch loss 1.122202, batch nll 0.590300, batch error rate 16.000000%\n",
      "At minibatch 74500, batch loss 1.024336, batch nll 0.491572, batch error rate 20.000000%\n",
      "At minibatch 74600, batch loss 0.806015, batch nll 0.273622, batch error rate 8.000000%\n",
      "At minibatch 74700, batch loss 1.145536, batch nll 0.612674, batch error rate 20.000000%\n",
      "At minibatch 74800, batch loss 1.016524, batch nll 0.484174, batch error rate 16.000000%\n",
      "At minibatch 74900, batch loss 1.127858, batch nll 0.595765, batch error rate 20.000000%\n",
      "At minibatch 75000, batch loss 0.960617, batch nll 0.428763, batch error rate 20.000000%\n",
      "At minibatch 75100, batch loss 0.757673, batch nll 0.226543, batch error rate 4.000000%\n",
      "At minibatch 75200, batch loss 1.216054, batch nll 0.685084, batch error rate 20.000000%\n",
      "After epoch 47: valid_err_rate: 24.380000% currently going to do 68 epochs\n",
      "After epoch 47: averaged train_err_rate: 14.270000% averaged train nll: 0.447100 averaged train loss: 0.978836\n",
      "At minibatch 75300, batch loss 0.924054, batch nll 0.391943, batch error rate 12.000000%\n",
      "At minibatch 75400, batch loss 0.903419, batch nll 0.370347, batch error rate 12.000000%\n",
      "At minibatch 75500, batch loss 0.978316, batch nll 0.444897, batch error rate 12.000000%\n",
      "At minibatch 75600, batch loss 1.100086, batch nll 0.565852, batch error rate 20.000000%\n",
      "At minibatch 75700, batch loss 0.986752, batch nll 0.451313, batch error rate 12.000000%\n",
      "At minibatch 75800, batch loss 0.930260, batch nll 0.393944, batch error rate 20.000000%\n",
      "At minibatch 75900, batch loss 0.732577, batch nll 0.196655, batch error rate 4.000000%\n",
      "At minibatch 76000, batch loss 0.932608, batch nll 0.396571, batch error rate 12.000000%\n",
      "At minibatch 76100, batch loss 1.006865, batch nll 0.470839, batch error rate 12.000000%\n",
      "At minibatch 76200, batch loss 1.127741, batch nll 0.592692, batch error rate 20.000000%\n",
      "At minibatch 76300, batch loss 0.876896, batch nll 0.341636, batch error rate 8.000000%\n",
      "At minibatch 76400, batch loss 1.131631, batch nll 0.596792, batch error rate 20.000000%\n",
      "At minibatch 76500, batch loss 1.088442, batch nll 0.554290, batch error rate 16.000000%\n",
      "At minibatch 76600, batch loss 0.947755, batch nll 0.413476, batch error rate 16.000000%\n",
      "At minibatch 76700, batch loss 1.073990, batch nll 0.539856, batch error rate 16.000000%\n",
      "At minibatch 76800, batch loss 1.126414, batch nll 0.592592, batch error rate 20.000000%\n",
      "After epoch 48: valid_err_rate: 25.290000% currently going to do 68 epochs\n",
      "After epoch 48: averaged train_err_rate: 13.827500% averaged train nll: 0.437566 averaged train loss: 0.972069\n",
      "At minibatch 76900, batch loss 0.811373, batch nll 0.276454, batch error rate 12.000000%\n",
      "At minibatch 77000, batch loss 1.385805, batch nll 0.850489, batch error rate 24.000000%\n",
      "At minibatch 77100, batch loss 0.908701, batch nll 0.372263, batch error rate 12.000000%\n",
      "At minibatch 77200, batch loss 0.845374, batch nll 0.308717, batch error rate 8.000000%\n",
      "At minibatch 77300, batch loss 1.259207, batch nll 0.722155, batch error rate 24.000000%\n",
      "At minibatch 77400, batch loss 1.265175, batch nll 0.727564, batch error rate 36.000000%\n",
      "At minibatch 77500, batch loss 0.986683, batch nll 0.449229, batch error rate 16.000000%\n",
      "At minibatch 77600, batch loss 0.990838, batch nll 0.453545, batch error rate 12.000000%\n",
      "At minibatch 77700, batch loss 1.131491, batch nll 0.593944, batch error rate 28.000000%\n",
      "At minibatch 77800, batch loss 0.861764, batch nll 0.323989, batch error rate 16.000000%\n",
      "At minibatch 77900, batch loss 1.001595, batch nll 0.464057, batch error rate 16.000000%\n",
      "At minibatch 78000, batch loss 1.174501, batch nll 0.636837, batch error rate 24.000000%\n",
      "At minibatch 78100, batch loss 1.007326, batch nll 0.469618, batch error rate 4.000000%\n",
      "At minibatch 78200, batch loss 0.870966, batch nll 0.333374, batch error rate 20.000000%\n",
      "At minibatch 78300, batch loss 1.042988, batch nll 0.505883, batch error rate 16.000000%\n",
      "At minibatch 78400, batch loss 1.018323, batch nll 0.481216, batch error rate 16.000000%\n",
      "After epoch 49: valid_err_rate: 25.540000% currently going to do 68 epochs\n",
      "After epoch 49: averaged train_err_rate: 13.745000% averaged train nll: 0.433049 averaged train loss: 0.970036\n",
      "At minibatch 78500, batch loss 0.994447, batch nll 0.456238, batch error rate 20.000000%\n",
      "At minibatch 78600, batch loss 0.864931, batch nll 0.325507, batch error rate 8.000000%\n",
      "At minibatch 78700, batch loss 1.037949, batch nll 0.497663, batch error rate 16.000000%\n",
      "At minibatch 78800, batch loss 1.102484, batch nll 0.561656, batch error rate 16.000000%\n",
      "At minibatch 78900, batch loss 0.948914, batch nll 0.407599, batch error rate 12.000000%\n",
      "At minibatch 79000, batch loss 0.874142, batch nll 0.333015, batch error rate 12.000000%\n",
      "At minibatch 79100, batch loss 0.982259, batch nll 0.440441, batch error rate 8.000000%\n",
      "At minibatch 79200, batch loss 1.042858, batch nll 0.500924, batch error rate 24.000000%\n",
      "At minibatch 79300, batch loss 1.341463, batch nll 0.799154, batch error rate 32.000000%\n",
      "At minibatch 79400, batch loss 1.001256, batch nll 0.459282, batch error rate 8.000000%\n",
      "At minibatch 79500, batch loss 0.970901, batch nll 0.429525, batch error rate 16.000000%\n",
      "At minibatch 79600, batch loss 0.931628, batch nll 0.390424, batch error rate 8.000000%\n",
      "At minibatch 79700, batch loss 0.963421, batch nll 0.423107, batch error rate 24.000000%\n",
      "At minibatch 79800, batch loss 0.741391, batch nll 0.201454, batch error rate 8.000000%\n",
      "At minibatch 79900, batch loss 1.218719, batch nll 0.679281, batch error rate 16.000000%\n",
      "At minibatch 80000, batch loss 0.933839, batch nll 0.394759, batch error rate 16.000000%\n",
      "After epoch 50: valid_err_rate: 24.390000% currently going to do 68 epochs\n",
      "After epoch 50: averaged train_err_rate: 13.517500% averaged train nll: 0.426236 averaged train loss: 0.966833\n",
      "At minibatch 80100, batch loss 0.784582, batch nll 0.244088, batch error rate 12.000000%\n",
      "At minibatch 80200, batch loss 0.957172, batch nll 0.416246, batch error rate 12.000000%\n",
      "At minibatch 80300, batch loss 1.017904, batch nll 0.476363, batch error rate 16.000000%\n",
      "At minibatch 80400, batch loss 0.836844, batch nll 0.294698, batch error rate 4.000000%\n",
      "At minibatch 80500, batch loss 0.818515, batch nll 0.276091, batch error rate 0.000000%\n",
      "At minibatch 80600, batch loss 0.831473, batch nll 0.288666, batch error rate 12.000000%\n",
      "At minibatch 80700, batch loss 0.845648, batch nll 0.302360, batch error rate 4.000000%\n",
      "At minibatch 80800, batch loss 0.863127, batch nll 0.320298, batch error rate 8.000000%\n",
      "At minibatch 80900, batch loss 1.115923, batch nll 0.572030, batch error rate 28.000000%\n",
      "At minibatch 81000, batch loss 0.989673, batch nll 0.445785, batch error rate 20.000000%\n",
      "At minibatch 81100, batch loss 0.819261, batch nll 0.275735, batch error rate 8.000000%\n",
      "At minibatch 81200, batch loss 1.000038, batch nll 0.457309, batch error rate 24.000000%\n",
      "At minibatch 81300, batch loss 0.945437, batch nll 0.403361, batch error rate 4.000000%\n",
      "At minibatch 81400, batch loss 1.173426, batch nll 0.631265, batch error rate 20.000000%\n",
      "At minibatch 81500, batch loss 1.149288, batch nll 0.607460, batch error rate 24.000000%\n",
      "At minibatch 81600, batch loss 1.241764, batch nll 0.699699, batch error rate 28.000000%\n",
      "After epoch 51: valid_err_rate: 24.840000% currently going to do 68 epochs\n",
      "After epoch 51: averaged train_err_rate: 13.265000% averaged train nll: 0.424268 averaged train loss: 0.966616\n",
      "At minibatch 81700, batch loss 0.954353, batch nll 0.411407, batch error rate 16.000000%\n",
      "At minibatch 81800, batch loss 0.793877, batch nll 0.249835, batch error rate 8.000000%\n",
      "At minibatch 81900, batch loss 0.838533, batch nll 0.293902, batch error rate 8.000000%\n",
      "At minibatch 82000, batch loss 0.879757, batch nll 0.334793, batch error rate 12.000000%\n",
      "At minibatch 82100, batch loss 0.882579, batch nll 0.337337, batch error rate 8.000000%\n",
      "At minibatch 82200, batch loss 1.105877, batch nll 0.560582, batch error rate 12.000000%\n",
      "At minibatch 82300, batch loss 0.983140, batch nll 0.438140, batch error rate 12.000000%\n",
      "At minibatch 82400, batch loss 0.889198, batch nll 0.344389, batch error rate 8.000000%\n",
      "At minibatch 82500, batch loss 1.160927, batch nll 0.615968, batch error rate 24.000000%\n",
      "At minibatch 82600, batch loss 1.120765, batch nll 0.575487, batch error rate 20.000000%\n",
      "At minibatch 82700, batch loss 0.977981, batch nll 0.432840, batch error rate 12.000000%\n",
      "At minibatch 82800, batch loss 0.960920, batch nll 0.415752, batch error rate 16.000000%\n",
      "At minibatch 82900, batch loss 1.186984, batch nll 0.642308, batch error rate 16.000000%\n",
      "At minibatch 83000, batch loss 1.012233, batch nll 0.467973, batch error rate 8.000000%\n",
      "At minibatch 83100, batch loss 0.924083, batch nll 0.379577, batch error rate 12.000000%\n",
      "At minibatch 83200, batch loss 1.303416, batch nll 0.759238, batch error rate 24.000000%\n",
      "After epoch 52: valid_err_rate: 24.320000% currently going to do 68 epochs\n",
      "After epoch 52: averaged train_err_rate: 13.030000% averaged train nll: 0.414183 averaged train loss: 0.958819\n",
      "At minibatch 83300, batch loss 0.966072, batch nll 0.420747, batch error rate 12.000000%\n",
      "At minibatch 83400, batch loss 1.067003, batch nll 0.521022, batch error rate 16.000000%\n",
      "At minibatch 83500, batch loss 0.896966, batch nll 0.350549, batch error rate 8.000000%\n",
      "At minibatch 83600, batch loss 1.044617, batch nll 0.497452, batch error rate 20.000000%\n",
      "At minibatch 83700, batch loss 0.965053, batch nll 0.417520, batch error rate 20.000000%\n",
      "At minibatch 83800, batch loss 0.929520, batch nll 0.381846, batch error rate 12.000000%\n",
      "At minibatch 83900, batch loss 1.199337, batch nll 0.651141, batch error rate 28.000000%\n",
      "At minibatch 84000, batch loss 1.009993, batch nll 0.462099, batch error rate 24.000000%\n",
      "At minibatch 84100, batch loss 1.061362, batch nll 0.513657, batch error rate 16.000000%\n",
      "At minibatch 84200, batch loss 0.946592, batch nll 0.398952, batch error rate 12.000000%\n",
      "At minibatch 84300, batch loss 0.712543, batch nll 0.164772, batch error rate 0.000000%\n",
      "At minibatch 84400, batch loss 1.044523, batch nll 0.496458, batch error rate 16.000000%\n",
      "At minibatch 84500, batch loss 1.101960, batch nll 0.554381, batch error rate 16.000000%\n",
      "At minibatch 84600, batch loss 1.101458, batch nll 0.554287, batch error rate 16.000000%\n",
      "At minibatch 84700, batch loss 1.391523, batch nll 0.844616, batch error rate 20.000000%\n",
      "At minibatch 84800, batch loss 0.889151, batch nll 0.342598, batch error rate 4.000000%\n",
      "After epoch 53: valid_err_rate: 24.490000% currently going to do 68 epochs\n",
      "After epoch 53: averaged train_err_rate: 12.535000% averaged train nll: 0.407070 averaged train loss: 0.954229\n",
      "At minibatch 84900, batch loss 0.918837, batch nll 0.371368, batch error rate 4.000000%\n",
      "At minibatch 85000, batch loss 0.877355, batch nll 0.329180, batch error rate 8.000000%\n",
      "At minibatch 85100, batch loss 0.876573, batch nll 0.327825, batch error rate 12.000000%\n",
      "At minibatch 85200, batch loss 0.761849, batch nll 0.212824, batch error rate 0.000000%\n",
      "At minibatch 85300, batch loss 1.019618, batch nll 0.470448, batch error rate 20.000000%\n",
      "At minibatch 85400, batch loss 0.939609, batch nll 0.390066, batch error rate 8.000000%\n",
      "At minibatch 85500, batch loss 0.985065, batch nll 0.434816, batch error rate 20.000000%\n",
      "At minibatch 85600, batch loss 0.985422, batch nll 0.434722, batch error rate 12.000000%\n",
      "At minibatch 85700, batch loss 0.797229, batch nll 0.246890, batch error rate 8.000000%\n",
      "At minibatch 85800, batch loss 0.868433, batch nll 0.317926, batch error rate 12.000000%\n",
      "At minibatch 85900, batch loss 1.192001, batch nll 0.641942, batch error rate 24.000000%\n",
      "At minibatch 86000, batch loss 0.917409, batch nll 0.367518, batch error rate 8.000000%\n",
      "At minibatch 86100, batch loss 0.962681, batch nll 0.412577, batch error rate 8.000000%\n",
      "At minibatch 86200, batch loss 1.264720, batch nll 0.715264, batch error rate 20.000000%\n",
      "At minibatch 86300, batch loss 0.958056, batch nll 0.409365, batch error rate 12.000000%\n",
      "At minibatch 86400, batch loss 0.995287, batch nll 0.446837, batch error rate 8.000000%\n",
      "After epoch 54: valid_err_rate: 24.350000% currently going to do 68 epochs\n",
      "After epoch 54: averaged train_err_rate: 12.600000% averaged train nll: 0.403500 averaged train loss: 0.952822\n",
      "At minibatch 86500, batch loss 0.716951, batch nll 0.167877, batch error rate 0.000000%\n",
      "At minibatch 86600, batch loss 0.771517, batch nll 0.221617, batch error rate 4.000000%\n",
      "At minibatch 86700, batch loss 0.974488, batch nll 0.423910, batch error rate 12.000000%\n",
      "At minibatch 86800, batch loss 0.799161, batch nll 0.248423, batch error rate 12.000000%\n",
      "At minibatch 86900, batch loss 0.881546, batch nll 0.330100, batch error rate 8.000000%\n",
      "At minibatch 87000, batch loss 0.871826, batch nll 0.319615, batch error rate 8.000000%\n",
      "At minibatch 87100, batch loss 0.953860, batch nll 0.401692, batch error rate 8.000000%\n",
      "At minibatch 87200, batch loss 1.065716, batch nll 0.513486, batch error rate 20.000000%\n",
      "At minibatch 87300, batch loss 0.809545, batch nll 0.257422, batch error rate 8.000000%\n",
      "At minibatch 87400, batch loss 1.039926, batch nll 0.488242, batch error rate 16.000000%\n",
      "At minibatch 87500, batch loss 1.365840, batch nll 0.813419, batch error rate 24.000000%\n",
      "At minibatch 87600, batch loss 0.999906, batch nll 0.447860, batch error rate 8.000000%\n",
      "At minibatch 87700, batch loss 0.831862, batch nll 0.279744, batch error rate 8.000000%\n",
      "At minibatch 87800, batch loss 1.095118, batch nll 0.543101, batch error rate 20.000000%\n",
      "At minibatch 87900, batch loss 1.027234, batch nll 0.475534, batch error rate 20.000000%\n",
      "At minibatch 88000, batch loss 0.947527, batch nll 0.396720, batch error rate 8.000000%\n",
      "After epoch 55: valid_err_rate: 25.710000% currently going to do 68 epochs\n",
      "After epoch 55: averaged train_err_rate: 12.340000% averaged train nll: 0.398192 averaged train loss: 0.949583\n",
      "At minibatch 88100, batch loss 0.929992, batch nll 0.377706, batch error rate 16.000000%\n",
      "At minibatch 88200, batch loss 0.889192, batch nll 0.336645, batch error rate 8.000000%\n",
      "At minibatch 88300, batch loss 0.931725, batch nll 0.378068, batch error rate 20.000000%\n",
      "At minibatch 88400, batch loss 1.059551, batch nll 0.505490, batch error rate 16.000000%\n",
      "At minibatch 88500, batch loss 0.926327, batch nll 0.372010, batch error rate 8.000000%\n",
      "At minibatch 88600, batch loss 0.902276, batch nll 0.348016, batch error rate 8.000000%\n",
      "At minibatch 88700, batch loss 1.097884, batch nll 0.543441, batch error rate 20.000000%\n",
      "At minibatch 88800, batch loss 0.737522, batch nll 0.183250, batch error rate 4.000000%\n",
      "At minibatch 88900, batch loss 0.872116, batch nll 0.317895, batch error rate 8.000000%\n",
      "At minibatch 89000, batch loss 1.127914, batch nll 0.573686, batch error rate 16.000000%\n",
      "At minibatch 89100, batch loss 0.919120, batch nll 0.365541, batch error rate 8.000000%\n",
      "At minibatch 89200, batch loss 0.915572, batch nll 0.361607, batch error rate 8.000000%\n",
      "At minibatch 89300, batch loss 0.944374, batch nll 0.390346, batch error rate 16.000000%\n",
      "At minibatch 89400, batch loss 0.861242, batch nll 0.307277, batch error rate 4.000000%\n",
      "At minibatch 89500, batch loss 0.835724, batch nll 0.282347, batch error rate 12.000000%\n",
      "At minibatch 89600, batch loss 0.962151, batch nll 0.409040, batch error rate 16.000000%\n",
      "After epoch 56: valid_err_rate: 24.670000% currently going to do 68 epochs\n",
      "After epoch 56: averaged train_err_rate: 11.837500% averaged train nll: 0.387970 averaged train loss: 0.941725\n",
      "At minibatch 89700, batch loss 1.034165, batch nll 0.480530, batch error rate 16.000000%\n",
      "At minibatch 89800, batch loss 1.095695, batch nll 0.541817, batch error rate 20.000000%\n",
      "At minibatch 89900, batch loss 0.859573, batch nll 0.305111, batch error rate 4.000000%\n",
      "At minibatch 90000, batch loss 1.224658, batch nll 0.670137, batch error rate 20.000000%\n",
      "At minibatch 90100, batch loss 0.997078, batch nll 0.442108, batch error rate 8.000000%\n",
      "At minibatch 90200, batch loss 1.174689, batch nll 0.619338, batch error rate 16.000000%\n",
      "At minibatch 90300, batch loss 0.874507, batch nll 0.318773, batch error rate 8.000000%\n",
      "At minibatch 90400, batch loss 1.013345, batch nll 0.457387, batch error rate 16.000000%\n",
      "At minibatch 90500, batch loss 0.791037, batch nll 0.234816, batch error rate 0.000000%\n",
      "At minibatch 90600, batch loss 0.731568, batch nll 0.175772, batch error rate 8.000000%\n",
      "At minibatch 90700, batch loss 0.909468, batch nll 0.354043, batch error rate 12.000000%\n",
      "At minibatch 90800, batch loss 1.004825, batch nll 0.450084, batch error rate 20.000000%\n",
      "At minibatch 90900, batch loss 1.026088, batch nll 0.472038, batch error rate 16.000000%\n",
      "At minibatch 91000, batch loss 1.181123, batch nll 0.627152, batch error rate 16.000000%\n",
      "At minibatch 91100, batch loss 0.860993, batch nll 0.307047, batch error rate 0.000000%\n",
      "At minibatch 91200, batch loss 0.805651, batch nll 0.251762, batch error rate 12.000000%\n",
      "After epoch 57: valid_err_rate: 24.740000% currently going to do 68 epochs\n",
      "After epoch 57: averaged train_err_rate: 11.755000% averaged train nll: 0.382478 averaged train loss: 0.937248\n",
      "At minibatch 91300, batch loss 0.967327, batch nll 0.412653, batch error rate 12.000000%\n",
      "At minibatch 91400, batch loss 1.277904, batch nll 0.722574, batch error rate 24.000000%\n",
      "At minibatch 91500, batch loss 0.819925, batch nll 0.264016, batch error rate 8.000000%\n",
      "At minibatch 91600, batch loss 0.982998, batch nll 0.426512, batch error rate 16.000000%\n",
      "At minibatch 91700, batch loss 1.121147, batch nll 0.564515, batch error rate 12.000000%\n",
      "At minibatch 91800, batch loss 1.155202, batch nll 0.598426, batch error rate 28.000000%\n",
      "At minibatch 91900, batch loss 0.896097, batch nll 0.339236, batch error rate 8.000000%\n",
      "At minibatch 92000, batch loss 0.895656, batch nll 0.338904, batch error rate 16.000000%\n",
      "At minibatch 92100, batch loss 1.100092, batch nll 0.542909, batch error rate 8.000000%\n",
      "At minibatch 92200, batch loss 0.999334, batch nll 0.441718, batch error rate 12.000000%\n",
      "At minibatch 92300, batch loss 0.868354, batch nll 0.310355, batch error rate 4.000000%\n",
      "At minibatch 92400, batch loss 0.916654, batch nll 0.359175, batch error rate 16.000000%\n",
      "At minibatch 92500, batch loss 0.960633, batch nll 0.402917, batch error rate 12.000000%\n",
      "At minibatch 92600, batch loss 1.077894, batch nll 0.520429, batch error rate 20.000000%\n",
      "At minibatch 92700, batch loss 0.961769, batch nll 0.404387, batch error rate 8.000000%\n",
      "At minibatch 92800, batch loss 1.009596, batch nll 0.452298, batch error rate 16.000000%\n",
      "After epoch 58: valid_err_rate: 23.880000% currently going to do 68 epochs\n",
      "After epoch 58: averaged train_err_rate: 11.582500% averaged train nll: 0.379748 averaged train loss: 0.936517\n",
      "At minibatch 92900, batch loss 0.895296, batch nll 0.337299, batch error rate 8.000000%\n",
      "At minibatch 93000, batch loss 0.980157, batch nll 0.422117, batch error rate 12.000000%\n",
      "At minibatch 93100, batch loss 1.040736, batch nll 0.482421, batch error rate 16.000000%\n",
      "At minibatch 93200, batch loss 0.995862, batch nll 0.437436, batch error rate 12.000000%\n",
      "At minibatch 93300, batch loss 0.934454, batch nll 0.375815, batch error rate 8.000000%\n",
      "At minibatch 93400, batch loss 0.939421, batch nll 0.379873, batch error rate 12.000000%\n",
      "At minibatch 93500, batch loss 0.979487, batch nll 0.419979, batch error rate 16.000000%\n",
      "At minibatch 93600, batch loss 1.117948, batch nll 0.558298, batch error rate 16.000000%\n",
      "At minibatch 93700, batch loss 1.250748, batch nll 0.691246, batch error rate 28.000000%\n",
      "At minibatch 93800, batch loss 0.984364, batch nll 0.424952, batch error rate 16.000000%\n",
      "At minibatch 93900, batch loss 1.116198, batch nll 0.556598, batch error rate 16.000000%\n",
      "At minibatch 94000, batch loss 1.073838, batch nll 0.514565, batch error rate 12.000000%\n",
      "At minibatch 94100, batch loss 0.996626, batch nll 0.437361, batch error rate 16.000000%\n",
      "At minibatch 94200, batch loss 0.821015, batch nll 0.261371, batch error rate 12.000000%\n",
      "At minibatch 94300, batch loss 1.064220, batch nll 0.504473, batch error rate 12.000000%\n",
      "At minibatch 94400, batch loss 0.824421, batch nll 0.265067, batch error rate 4.000000%\n",
      "After epoch 59: valid_err_rate: 24.580000% currently going to do 68 epochs\n",
      "After epoch 59: averaged train_err_rate: 11.100000% averaged train nll: 0.372686 averaged train loss: 0.931778\n",
      "At minibatch 94500, batch loss 0.939145, batch nll 0.379404, batch error rate 12.000000%\n",
      "At minibatch 94600, batch loss 1.067050, batch nll 0.506984, batch error rate 16.000000%\n",
      "At minibatch 94700, batch loss 0.950937, batch nll 0.390576, batch error rate 12.000000%\n",
      "At minibatch 94800, batch loss 0.981937, batch nll 0.421785, batch error rate 20.000000%\n",
      "At minibatch 94900, batch loss 0.773281, batch nll 0.212499, batch error rate 0.000000%\n",
      "At minibatch 95000, batch loss 0.860282, batch nll 0.299283, batch error rate 4.000000%\n",
      "At minibatch 95100, batch loss 0.922842, batch nll 0.362061, batch error rate 8.000000%\n",
      "At minibatch 95200, batch loss 1.018460, batch nll 0.457593, batch error rate 8.000000%\n",
      "At minibatch 95300, batch loss 0.859592, batch nll 0.298577, batch error rate 0.000000%\n",
      "At minibatch 95400, batch loss 0.773604, batch nll 0.212601, batch error rate 0.000000%\n",
      "At minibatch 95500, batch loss 0.880405, batch nll 0.319482, batch error rate 12.000000%\n",
      "At minibatch 95600, batch loss 0.829373, batch nll 0.268504, batch error rate 4.000000%\n",
      "At minibatch 95700, batch loss 1.060093, batch nll 0.499196, batch error rate 12.000000%\n",
      "At minibatch 95800, batch loss 0.914150, batch nll 0.352860, batch error rate 12.000000%\n",
      "At minibatch 95900, batch loss 0.969083, batch nll 0.407613, batch error rate 20.000000%\n",
      "At minibatch 96000, batch loss 0.858325, batch nll 0.297287, batch error rate 8.000000%\n",
      "After epoch 60: valid_err_rate: 24.830000% currently going to do 68 epochs\n",
      "After epoch 60: averaged train_err_rate: 11.020000% averaged train nll: 0.368375 averaged train loss: 0.929120\n",
      "At minibatch 96100, batch loss 0.895410, batch nll 0.333232, batch error rate 12.000000%\n",
      "At minibatch 96200, batch loss 1.065210, batch nll 0.503191, batch error rate 16.000000%\n",
      "At minibatch 96300, batch loss 0.990949, batch nll 0.428844, batch error rate 24.000000%\n",
      "At minibatch 96400, batch loss 0.944865, batch nll 0.382461, batch error rate 12.000000%\n",
      "At minibatch 96500, batch loss 1.037684, batch nll 0.475768, batch error rate 16.000000%\n",
      "At minibatch 96600, batch loss 0.776402, batch nll 0.214354, batch error rate 8.000000%\n",
      "At minibatch 96700, batch loss 0.925020, batch nll 0.363160, batch error rate 12.000000%\n",
      "At minibatch 96800, batch loss 1.082134, batch nll 0.520092, batch error rate 20.000000%\n",
      "At minibatch 96900, batch loss 0.734275, batch nll 0.172181, batch error rate 0.000000%\n",
      "At minibatch 97000, batch loss 1.066851, batch nll 0.504850, batch error rate 12.000000%\n",
      "At minibatch 97100, batch loss 0.764674, batch nll 0.202417, batch error rate 4.000000%\n",
      "At minibatch 97200, batch loss 0.905846, batch nll 0.343689, batch error rate 8.000000%\n",
      "At minibatch 97300, batch loss 1.016681, batch nll 0.454245, batch error rate 20.000000%\n",
      "At minibatch 97400, batch loss 0.816588, batch nll 0.254244, batch error rate 4.000000%\n",
      "At minibatch 97500, batch loss 0.876643, batch nll 0.314926, batch error rate 8.000000%\n",
      "At minibatch 97600, batch loss 0.749132, batch nll 0.187250, batch error rate 4.000000%\n",
      "After epoch 61: valid_err_rate: 24.780000% currently going to do 68 epochs\n",
      "After epoch 61: averaged train_err_rate: 10.825000% averaged train nll: 0.360718 averaged train loss: 0.922797\n",
      "At minibatch 97700, batch loss 0.939373, batch nll 0.376936, batch error rate 8.000000%\n",
      "At minibatch 97800, batch loss 0.936268, batch nll 0.373202, batch error rate 16.000000%\n",
      "At minibatch 97900, batch loss 0.958041, batch nll 0.394826, batch error rate 12.000000%\n",
      "At minibatch 98000, batch loss 0.953811, batch nll 0.390404, batch error rate 12.000000%\n",
      "At minibatch 98100, batch loss 0.961618, batch nll 0.397961, batch error rate 12.000000%\n",
      "At minibatch 98200, batch loss 0.848334, batch nll 0.284803, batch error rate 8.000000%\n",
      "At minibatch 98300, batch loss 0.882285, batch nll 0.318779, batch error rate 4.000000%\n",
      "At minibatch 98400, batch loss 1.110492, batch nll 0.546979, batch error rate 24.000000%\n",
      "At minibatch 98500, batch loss 0.768314, batch nll 0.205066, batch error rate 8.000000%\n",
      "At minibatch 98600, batch loss 0.752034, batch nll 0.188814, batch error rate 4.000000%\n",
      "At minibatch 98700, batch loss 0.856253, batch nll 0.292516, batch error rate 12.000000%\n",
      "At minibatch 98800, batch loss 1.312556, batch nll 0.748741, batch error rate 24.000000%\n",
      "At minibatch 98900, batch loss 0.907302, batch nll 0.343306, batch error rate 12.000000%\n",
      "At minibatch 99000, batch loss 1.127437, batch nll 0.563734, batch error rate 12.000000%\n",
      "At minibatch 99100, batch loss 0.935927, batch nll 0.372629, batch error rate 8.000000%\n",
      "At minibatch 99200, batch loss 0.801335, batch nll 0.238249, batch error rate 4.000000%\n",
      "After epoch 62: valid_err_rate: 24.730000% currently going to do 68 epochs\n",
      "After epoch 62: averaged train_err_rate: 10.852500% averaged train nll: 0.361669 averaged train loss: 0.925037\n",
      "At minibatch 99300, batch loss 0.832971, batch nll 0.269224, batch error rate 8.000000%\n",
      "At minibatch 99400, batch loss 0.908969, batch nll 0.344765, batch error rate 8.000000%\n",
      "At minibatch 99500, batch loss 0.907512, batch nll 0.342634, batch error rate 12.000000%\n",
      "At minibatch 99600, batch loss 1.092305, batch nll 0.527905, batch error rate 24.000000%\n",
      "At minibatch 99700, batch loss 0.917417, batch nll 0.352728, batch error rate 12.000000%\n",
      "At minibatch 99800, batch loss 0.843182, batch nll 0.277893, batch error rate 8.000000%\n",
      "At minibatch 99900, batch loss 0.962319, batch nll 0.397033, batch error rate 16.000000%\n",
      "At minibatch 100000, batch loss 0.775111, batch nll 0.209409, batch error rate 4.000000%\n",
      "At minibatch 100100, batch loss 0.909768, batch nll 0.344235, batch error rate 4.000000%\n",
      "At minibatch 100200, batch loss 1.034943, batch nll 0.469317, batch error rate 20.000000%\n",
      "At minibatch 100300, batch loss 0.928679, batch nll 0.363054, batch error rate 12.000000%\n",
      "At minibatch 100400, batch loss 0.854203, batch nll 0.288203, batch error rate 4.000000%\n",
      "At minibatch 100500, batch loss 1.070541, batch nll 0.504999, batch error rate 16.000000%\n",
      "At minibatch 100600, batch loss 1.157157, batch nll 0.592281, batch error rate 20.000000%\n",
      "At minibatch 100700, batch loss 0.967772, batch nll 0.403167, batch error rate 12.000000%\n",
      "At minibatch 100800, batch loss 1.160238, batch nll 0.595359, batch error rate 24.000000%\n",
      "After epoch 63: valid_err_rate: 25.090000% currently going to do 68 epochs\n",
      "After epoch 63: averaged train_err_rate: 10.317500% averaged train nll: 0.353126 averaged train loss: 0.918113\n",
      "At minibatch 100900, batch loss 0.823102, batch nll 0.257572, batch error rate 8.000000%\n",
      "At minibatch 101000, batch loss 0.997739, batch nll 0.432659, batch error rate 16.000000%\n",
      "At minibatch 101100, batch loss 0.875498, batch nll 0.309763, batch error rate 4.000000%\n",
      "At minibatch 101200, batch loss 1.032646, batch nll 0.466226, batch error rate 20.000000%\n",
      "At minibatch 101300, batch loss 1.036260, batch nll 0.469559, batch error rate 16.000000%\n",
      "At minibatch 101400, batch loss 0.758500, batch nll 0.191307, batch error rate 0.000000%\n",
      "At minibatch 101500, batch loss 0.891059, batch nll 0.324354, batch error rate 16.000000%\n",
      "At minibatch 101600, batch loss 0.886075, batch nll 0.319469, batch error rate 12.000000%\n",
      "At minibatch 101700, batch loss 1.102436, batch nll 0.536120, batch error rate 16.000000%\n",
      "At minibatch 101800, batch loss 0.919354, batch nll 0.352696, batch error rate 8.000000%\n",
      "At minibatch 101900, batch loss 0.986660, batch nll 0.419645, batch error rate 16.000000%\n",
      "At minibatch 102000, batch loss 0.913720, batch nll 0.347147, batch error rate 12.000000%\n",
      "At minibatch 102100, batch loss 1.010199, batch nll 0.443801, batch error rate 8.000000%\n",
      "At minibatch 102200, batch loss 0.831916, batch nll 0.265140, batch error rate 4.000000%\n",
      "At minibatch 102300, batch loss 1.117898, batch nll 0.551184, batch error rate 20.000000%\n",
      "At minibatch 102400, batch loss 0.836941, batch nll 0.270772, batch error rate 12.000000%\n",
      "After epoch 64: valid_err_rate: 24.870000% currently going to do 68 epochs\n",
      "After epoch 64: averaged train_err_rate: 10.197500% averaged train nll: 0.349907 averaged train loss: 0.916260\n",
      "At minibatch 102500, batch loss 1.080258, batch nll 0.513434, batch error rate 16.000000%\n",
      "At minibatch 102600, batch loss 0.756405, batch nll 0.189521, batch error rate 0.000000%\n",
      "At minibatch 102700, batch loss 0.924852, batch nll 0.357775, batch error rate 16.000000%\n",
      "At minibatch 102800, batch loss 0.884735, batch nll 0.317421, batch error rate 12.000000%\n",
      "At minibatch 102900, batch loss 1.113914, batch nll 0.546524, batch error rate 24.000000%\n",
      "At minibatch 103000, batch loss 1.160487, batch nll 0.592797, batch error rate 16.000000%\n",
      "At minibatch 103100, batch loss 0.927394, batch nll 0.359389, batch error rate 12.000000%\n",
      "At minibatch 103200, batch loss 0.868328, batch nll 0.300007, batch error rate 8.000000%\n",
      "At minibatch 103300, batch loss 0.922951, batch nll 0.354234, batch error rate 20.000000%\n",
      "At minibatch 103400, batch loss 0.778888, batch nll 0.209849, batch error rate 4.000000%\n",
      "At minibatch 103500, batch loss 0.889215, batch nll 0.320385, batch error rate 8.000000%\n",
      "At minibatch 103600, batch loss 1.182094, batch nll 0.613427, batch error rate 28.000000%\n",
      "At minibatch 103700, batch loss 0.941637, batch nll 0.372761, batch error rate 4.000000%\n",
      "At minibatch 103800, batch loss 0.989988, batch nll 0.421763, batch error rate 16.000000%\n",
      "At minibatch 103900, batch loss 0.970404, batch nll 0.402031, batch error rate 16.000000%\n",
      "At minibatch 104000, batch loss 0.952525, batch nll 0.384870, batch error rate 12.000000%\n",
      "After epoch 65: valid_err_rate: 24.800000% currently going to do 68 epochs\n",
      "After epoch 65: averaged train_err_rate: 9.890000% averaged train nll: 0.340641 averaged train loss: 0.908604\n",
      "At minibatch 104100, batch loss 0.981322, batch nll 0.413537, batch error rate 8.000000%\n",
      "At minibatch 104200, batch loss 0.853447, batch nll 0.285270, batch error rate 4.000000%\n",
      "At minibatch 104300, batch loss 0.919483, batch nll 0.350480, batch error rate 12.000000%\n",
      "At minibatch 104400, batch loss 0.717323, batch nll 0.148494, batch error rate 8.000000%\n",
      "At minibatch 104500, batch loss 0.889345, batch nll 0.319951, batch error rate 8.000000%\n",
      "At minibatch 104600, batch loss 0.885374, batch nll 0.316055, batch error rate 12.000000%\n",
      "At minibatch 104700, batch loss 0.988726, batch nll 0.419617, batch error rate 4.000000%\n",
      "At minibatch 104800, batch loss 0.889476, batch nll 0.320439, batch error rate 8.000000%\n",
      "At minibatch 104900, batch loss 0.882746, batch nll 0.313751, batch error rate 8.000000%\n",
      "At minibatch 105000, batch loss 0.770565, batch nll 0.201526, batch error rate 0.000000%\n",
      "At minibatch 105100, batch loss 1.011909, batch nll 0.442946, batch error rate 16.000000%\n",
      "At minibatch 105200, batch loss 0.866738, batch nll 0.297928, batch error rate 20.000000%\n",
      "At minibatch 105300, batch loss 1.077499, batch nll 0.508520, batch error rate 16.000000%\n",
      "At minibatch 105400, batch loss 0.937482, batch nll 0.368875, batch error rate 8.000000%\n",
      "At minibatch 105500, batch loss 0.813994, batch nll 0.245383, batch error rate 8.000000%\n",
      "At minibatch 105600, batch loss 1.013020, batch nll 0.444238, batch error rate 8.000000%\n",
      "After epoch 66: valid_err_rate: 25.490000% currently going to do 68 epochs\n",
      "After epoch 66: averaged train_err_rate: 9.827500% averaged train nll: 0.340033 averaged train loss: 0.908846\n",
      "At minibatch 105700, batch loss 0.812640, batch nll 0.243116, batch error rate 8.000000%\n",
      "At minibatch 105800, batch loss 1.107459, batch nll 0.537850, batch error rate 20.000000%\n",
      "At minibatch 105900, batch loss 0.796327, batch nll 0.226507, batch error rate 8.000000%\n",
      "At minibatch 106000, batch loss 0.837552, batch nll 0.267937, batch error rate 4.000000%\n",
      "At minibatch 106100, batch loss 1.071166, batch nll 0.501448, batch error rate 16.000000%\n",
      "At minibatch 106200, batch loss 1.022425, batch nll 0.452485, batch error rate 12.000000%\n",
      "At minibatch 106300, batch loss 0.854243, batch nll 0.283973, batch error rate 4.000000%\n",
      "At minibatch 106400, batch loss 0.836345, batch nll 0.266032, batch error rate 8.000000%\n",
      "At minibatch 106500, batch loss 1.017280, batch nll 0.447149, batch error rate 20.000000%\n",
      "At minibatch 106600, batch loss 0.987871, batch nll 0.417425, batch error rate 12.000000%\n",
      "At minibatch 106700, batch loss 0.725914, batch nll 0.155594, batch error rate 0.000000%\n",
      "At minibatch 106800, batch loss 0.903239, batch nll 0.333045, batch error rate 12.000000%\n",
      "At minibatch 106900, batch loss 0.870182, batch nll 0.300351, batch error rate 4.000000%\n",
      "At minibatch 107000, batch loss 0.857801, batch nll 0.288108, batch error rate 4.000000%\n",
      "At minibatch 107100, batch loss 0.799877, batch nll 0.230545, batch error rate 8.000000%\n",
      "At minibatch 107200, batch loss 0.882250, batch nll 0.312529, batch error rate 4.000000%\n",
      "After epoch 67: valid_err_rate: 23.970000% currently going to do 68 epochs\n",
      "After epoch 67: averaged train_err_rate: 9.590000% averaged train nll: 0.336729 averaged train loss: 0.906598\n",
      "At minibatch 107300, batch loss 0.813594, batch nll 0.243252, batch error rate 4.000000%\n",
      "At minibatch 107400, batch loss 0.750681, batch nll 0.180052, batch error rate 4.000000%\n",
      "At minibatch 107500, batch loss 0.795157, batch nll 0.224286, batch error rate 4.000000%\n",
      "At minibatch 107600, batch loss 0.939916, batch nll 0.368781, batch error rate 8.000000%\n",
      "At minibatch 107700, batch loss 0.964889, batch nll 0.393223, batch error rate 20.000000%\n",
      "At minibatch 107800, batch loss 0.928162, batch nll 0.356425, batch error rate 16.000000%\n",
      "At minibatch 107900, batch loss 0.930125, batch nll 0.358511, batch error rate 20.000000%\n",
      "At minibatch 108000, batch loss 0.972675, batch nll 0.400888, batch error rate 12.000000%\n",
      "At minibatch 108100, batch loss 0.966820, batch nll 0.395155, batch error rate 12.000000%\n",
      "At minibatch 108200, batch loss 0.962034, batch nll 0.389899, batch error rate 16.000000%\n",
      "At minibatch 108300, batch loss 0.919885, batch nll 0.347946, batch error rate 20.000000%\n",
      "At minibatch 108400, batch loss 1.014074, batch nll 0.442103, batch error rate 12.000000%\n",
      "At minibatch 108500, batch loss 0.843632, batch nll 0.271730, batch error rate 8.000000%\n",
      "At minibatch 108600, batch loss 0.821561, batch nll 0.249780, batch error rate 4.000000%\n",
      "At minibatch 108700, batch loss 0.877681, batch nll 0.306129, batch error rate 8.000000%\n",
      "At minibatch 108800, batch loss 0.863114, batch nll 0.291886, batch error rate 12.000000%\n",
      "After epoch 68: valid_err_rate: 24.230000% currently going to do 68 epochs\n",
      "After epoch 68: averaged train_err_rate: 9.130000% averaged train nll: 0.327098 averaged train loss: 0.898580\n",
      "At minibatch 108900, batch loss 1.024921, batch nll 0.453606, batch error rate 16.000000%\n",
      "At minibatch 109000, batch loss 0.739165, batch nll 0.167539, batch error rate 0.000000%\n",
      "At minibatch 109100, batch loss 0.840628, batch nll 0.268999, batch error rate 8.000000%\n",
      "At minibatch 109200, batch loss 0.809047, batch nll 0.237106, batch error rate 4.000000%\n",
      "At minibatch 109300, batch loss 1.002230, batch nll 0.430025, batch error rate 16.000000%\n",
      "At minibatch 109400, batch loss 0.808156, batch nll 0.235654, batch error rate 8.000000%\n",
      "At minibatch 109500, batch loss 1.005055, batch nll 0.432437, batch error rate 12.000000%\n",
      "At minibatch 109600, batch loss 1.011954, batch nll 0.439276, batch error rate 16.000000%\n",
      "At minibatch 109700, batch loss 0.916486, batch nll 0.343599, batch error rate 8.000000%\n",
      "At minibatch 109800, batch loss 0.839801, batch nll 0.266822, batch error rate 8.000000%\n",
      "At minibatch 109900, batch loss 0.972326, batch nll 0.399861, batch error rate 12.000000%\n",
      "At minibatch 110000, batch loss 0.962212, batch nll 0.389819, batch error rate 16.000000%\n",
      "At minibatch 110100, batch loss 0.938266, batch nll 0.366186, batch error rate 12.000000%\n",
      "At minibatch 110200, batch loss 0.815354, batch nll 0.243447, batch error rate 12.000000%\n",
      "At minibatch 110300, batch loss 1.060486, batch nll 0.489021, batch error rate 20.000000%\n",
      "At minibatch 110400, batch loss 0.943725, batch nll 0.372339, batch error rate 8.000000%\n",
      "After epoch 69: valid_err_rate: 25.510000% currently going to do 68 epochs\n",
      "After epoch 69: averaged train_err_rate: 9.267500% averaged train nll: 0.324962 averaged train loss: 0.897069\n",
      "Setting network parameters from after epoch 45\n",
      "Test error rate: 0.249200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fcd50c8ff10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEDCAYAAAAvNJM9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcd56597f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# init training\n",
    "\n",
    "i=0\n",
    "e=0\n",
    "\n",
    "init_parameters()\n",
    "for v in velocities:\n",
    "    v.set_value(np.zeros_like(v.get_value()))\n",
    "\n",
    "best_valid_error_rate = np.inf\n",
    "best_params = snapshot_parameters()\n",
    "best_params_epoch = 0\n",
    "\n",
    "train_erros = []\n",
    "train_loss = []\n",
    "train_nll = []\n",
    "validation_errors = []\n",
    "\n",
    "number_of_epochs = 3\n",
    "patience_expansion = 1.5\n",
    "\n",
    "base_lrate = 1e-2\n",
    "K = 10000\n",
    "momentum=0.9\n",
    "\n",
    "# training loop\n",
    "\n",
    "try:\n",
    "    while e<number_of_epochs: #This loop goes over epochs\n",
    "        e += 1\n",
    "        #First train on all data from this batch\n",
    "        epoch_start_i = i\n",
    "        for X_batch, Y_batch in cifar10_train_stream.get_epoch_iterator(): \n",
    "            i += 1\n",
    "            lrate = base_lrate * K / np.maximum(K, i)\n",
    "\n",
    "            L, err_rate, nll, wdec = train_step(X_batch, Y_batch, lrate, momentum)\n",
    "\n",
    "            train_loss.append((i,L))\n",
    "            train_erros.append((i,err_rate))\n",
    "            train_nll.append((i,nll))\n",
    "            if i % 100 == 0:\n",
    "                print \"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate*100)\n",
    "\n",
    "        # After an epoch compute validation error\n",
    "        val_error_rate = compute_error_rate(cifar10_validation_stream)\n",
    "        if val_error_rate < best_valid_error_rate:\n",
    "            number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "            best_valid_error_rate = val_error_rate\n",
    "            best_params = snapshot_parameters()\n",
    "            best_params_epoch = e\n",
    "        validation_errors.append((i,val_error_rate))\n",
    "        print \"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" %(\n",
    "            e, val_error_rate*100, number_of_epochs)\n",
    "        print \"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" %(\n",
    "            e, np.mean(np.asarray(train_erros)[epoch_start_i:,1])*100, \n",
    "            np.mean(np.asarray(train_nll)[epoch_start_i:,1]),\n",
    "            np.mean(np.asarray(train_loss)[epoch_start_i:,1]))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print \"Setting network parameters from after epoch %d\" %(best_params_epoch)\n",
    "load_parameters(best_params)\n",
    "\n",
    "print \"Test error rate: %f\" % (compute_error_rate(cifar10_test_stream), )\n",
    "\n",
    "subplot(2,1,1)\n",
    "train_loss = np.array(train_loss)\n",
    "semilogy(train_loss[:,0], train_loss[:,1], label='batch train loss')\n",
    "legend()\n",
    "\n",
    "subplot(2,1,2)\n",
    "train_erros = np.array(train_erros)\n",
    "plot(train_erros[:,0], train_erros[:,1], label='batch train error rate')\n",
    "validation_errors = np.array(validation_errors)\n",
    "plot(validation_errors[:,0], validation_errors[:,1], label='validation error rate', color='r')\n",
    "ylim(0,0.4)\n",
    "legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
